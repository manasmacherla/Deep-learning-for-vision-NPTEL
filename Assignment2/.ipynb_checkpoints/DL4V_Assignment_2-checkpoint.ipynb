{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6ayCViEg4nh"
   },
   "source": [
    "#### **Welcome to Assignment 2 on Deep Learning for Computer Vision.**\n",
    "This assignment consists of three parts. Part-1 is based on the content you learned in Week-3 of course and Part-2 is based on the content you learned in Week-4 of the course. Part-3 is **un-graded** and mainly designed to help you flex the Deep Learning muscles grown in Part-2. \n",
    "\n",
    "Unlike the first two parts, you'll have to implement everything from scratch in Part-3. If you find answers to questions in Part-3, feel free to head out to the forums and discuss them with your classmates!\n",
    "\n",
    "#### **Instructions**\n",
    "1. Use Python 3.x to run this notebook\n",
    "2. Write your code only in between the lines 'YOUR CODE STARTS HERE' and 'YOUR CODE ENDS HERE'.\n",
    "you should not change anything else in the code cells, if you do, the answers you are supposed to get at the end of this assignment might be wrong.\n",
    "3. Read documentation of each function carefully.\n",
    "4. All the Best!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jca6tcpSh0EK"
   },
   "source": [
    "# Part-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5KURep5V_0th"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# %matplotlib inline uncomment this line if you're running this notebook on your local PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZEae-LT_5n-"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CODE\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSGQF0Zu9Vuo"
   },
   "source": [
    "### Question 1 : Line detection from a given image.\n",
    "\n",
    "\n",
    "Find the starting and ending point co-ordinates of detected lines of a given image using hough transform. \n",
    "\n",
    "Following criterion need to be satisfied to qualify as a line:\n",
    "\n",
    "(a) Minimum line length = 60;\n",
    "(b) Maximum allowed gap between line segments = 250;\n",
    "(c) Accumulator threshold parameter = 15  (only those lines are returned that get enough votes);\n",
    "(d) Distance resolution of the accumulator in pixels = 1;\n",
    "(e) Angle resolution of the accumulator in radians = pi/180\n",
    "\n",
    "\n",
    "Which of the following pairs of (start, end) co-ordinates of detected lines are true:\n",
    "\n",
    "1. line-1: (1,81), (27,9) ; line-2 : (92,0), (95,64)\n",
    "2. line-1: (2,79), (26,11) ; line-2 : (92,1), (94,67)\n",
    "3. line-1: (2,80), (26,10) ; line-2: (91,2), (96,65)\n",
    "4.  None of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwdLOgZs99bC"
   },
   "outputs": [],
   "source": [
    "#Read image \n",
    "img = cv2.imread('line.png', cv2.IMREAD_COLOR)\n",
    "\n",
    "# Visualize the input image\n",
    "plt.imshow(img)\n",
    "plt.title('Input Image')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#convert the image to gray-scale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Find the edges in the image using canny detector\n",
    "\n",
    "edges = cv2.Canny(gray, 50, 200)\n",
    "#plt.imshow(edges)\n",
    "#plt.show()\n",
    "\n",
    "####YOUR CODE STARTS HERE #####\n",
    "rho = 1\n",
    "theta = np.pi/180\n",
    "threshold = 15\n",
    "min_line_length = 60\n",
    "max_line_gap = 250\n",
    "line_image = np.copy(img)*0 #creating a blank to draw lines on\n",
    "\n",
    "lines = cv2.HoughLinesP(edges, rho, theta, threshold, np.array([]),min_line_length, max_line_gap)\n",
    "\n",
    "for line in lines:\n",
    "    for x1,y1,x2,y2 in line:\n",
    "        cv2.line(line_image, (x1,y1), (x2,y2), (255,0,0), 5)\n",
    "    \n",
    "img_line = cv2.addWeighted(img, 0.8, line_image, 1, 0)\n",
    "\n",
    "####YOUR CODE ENDS HERE #####\n",
    "plt.imshow(img_line)\n",
    "plt.title('Detected Line Image')\n",
    "plt.show()\n",
    "\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhErU_TY-PpU"
   },
   "source": [
    "##Question 2: Point matching using RANSAC\n",
    "\n",
    "Given two sets of points related by affine transformation(with an outlier rate), use the RANSAC method to estimate the Affine transformation parameters between them and the number of inliers(Matching points).\n",
    "\n",
    "Which of the following is the estimated number of inliers for an outlier rate of 0.9:\n",
    "\n",
    "1. 89\n",
    "2. 101\n",
    "3. 117\n",
    "4. 132"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "TYotkDxG-kkP",
    "outputId": "b164d043-d351-449e-8df9-ee3673a57658"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "# Affine Transform\n",
    "# |x'|  = |a, b| * |x|  +  |tx|\n",
    "# |y'|    |c, d|   |y|     |ty|\n",
    "# pts_t =    A   * pts_s  + t\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Test Class Affine\n",
    "\n",
    "class Affine_Transform():\n",
    "\n",
    "    def create_test_case(self, outlier_rate=0):\n",
    "        ''' CREATE_TEST_CASE\n",
    "\n",
    "            Randomly generate a test case of affine transformation.\n",
    "\n",
    "            Input arguments:\n",
    "\n",
    "            - outlier_rate : the percentage of outliers in test case,\n",
    "            default is 0\n",
    "\n",
    "            Outputs:\n",
    "\n",
    "            - pts_s : Source points that will be transformed\n",
    "            - pts_t : warped points\n",
    "            - A, t : parameters of affine transformation, A is a 2x2\n",
    "            matrix, t is a 2x1 vector, both of them are created randomly\n",
    "\n",
    "        '''\n",
    "\n",
    "        # Randomly generate affine transformation\n",
    "        # A is a 2x2 matrix, the range of each value is from -2 to 2\n",
    "        A = 4 * np.random.rand(2, 2) - 2\n",
    "\n",
    "        # % t is a 2x1 VECTOR, the range of each value is from -10 to 10\n",
    "        t = 20 * np.random.rand(2, 1) - 10\n",
    "\n",
    "        # Set the number of points in test case\n",
    "        num = 1000\n",
    "\n",
    "        # Compute the number of outliers and inliers respectively\n",
    "        outliers = int(np.round(num * outlier_rate))\n",
    "        inliers = int(num - outliers)\n",
    "\n",
    "        # Gernerate source points whose scope from (0,0) to (100, 100)\n",
    "        pts_s = 100 * np.random.rand(2, num)\n",
    "        # Initialize warped points matrix\n",
    "        pts_t = np.zeros((2, num))\n",
    "\n",
    "        # Compute inliers in warped points matrix by applying A and t\n",
    "        pts_t[:, :inliers] = np.dot(A, pts_s[:, :inliers]) + t\n",
    "\n",
    "        # Generate outliers in warped points matrix\n",
    "        pts_t[:, inliers:] = 100 * np.random.rand(2, outliers)\n",
    "\n",
    "        # Reset the order of warped points matrix,\n",
    "        # outliers and inliers will scatter randomly in test case\n",
    "        rnd_idx = np.random.permutation(num)\n",
    "        pts_s = pts_s[:, rnd_idx]\n",
    "        pts_t = pts_t[:, rnd_idx]\n",
    "\n",
    "        return A, t, pts_s, pts_t\n",
    "\n",
    "    def estimate_affine(self, pts_s, pts_t):\n",
    "        ''' ESTIMATE_AFFINE\n",
    "\n",
    "            Estimate affine transformation by the given points\n",
    "            correspondences.\n",
    "\n",
    "            Input arguments:\n",
    "            - pts_t : points in target image\n",
    "            - pts_s : points in source image\n",
    "\n",
    "            Outputs:\n",
    "\n",
    "            - A, t : the affine transformation, A is a 2x2 matrix\n",
    "            that indicates the rotation and scaling transformation,\n",
    "            t is a 2x1 vector determines the translation\n",
    "\n",
    "            Method:\n",
    "\n",
    "            To estimate an affine transformation between two images,\n",
    "            at least 3 corresponding points are needed.\n",
    "            In this case, 6-parameter affine transformation are taken into\n",
    "            consideration, which is shown as follows:\n",
    "\n",
    "            | x' | = | a b | * | x | + | tx |\n",
    "            | y' |   | c d |   | y |   | ty |\n",
    "\n",
    "            For 3 corresponding points, 6 equations can be formed as below:\n",
    "\n",
    "            | x1 y1 0  0  1 0 |       | a  |       | x1' |\n",
    "            | 0  0  x1 y1 0 1 |       | b  |       | y1' |\n",
    "            | x2 y2 0  0  1 0 |   *   | c  |   =   | x2' |\n",
    "            | 0  0  x2 y2 0 1 |       | d  |       | y2' |\n",
    "            | x3 y3 0  0  1 0 |       | tx |       | x3' |\n",
    "            | 0  0  x3 y3 0 1 |       | ty |       | y3' |\n",
    "\n",
    "            |------> M <------|   |-> theta <-|   |-> b <-|\n",
    "\n",
    "            Solve the equation to compute theta by:  theta = M \\ b\n",
    "            Thus, affine transformation can be obtained as:\n",
    "\n",
    "            A = | a b |     t = | tx |\n",
    "                | c d |         | ty |\n",
    "\n",
    "        '''\n",
    "\n",
    "        # Get the number of corresponding points\n",
    "        pts_num = pts_s.shape[1]\n",
    "\n",
    "        # Initialize the matrix M,\n",
    "        # M has 6 columns, since the affine transformation\n",
    "        # has 6 parameters in this case\n",
    "        M = np.zeros((2 * pts_num, 6))\n",
    "\n",
    "        for i in range(pts_num):\n",
    "            # Form the matrix m\n",
    "            temp = [[pts_s[0, i], pts_s[1, i], 0, 0, 1, 0],\n",
    "                    [0, 0, pts_s[0, i], pts_s[1, i], 0, 1]]\n",
    "            M[2 * i: 2 * i + 2, :] = np.array(temp)\n",
    "\n",
    "        # Form the matrix b,\n",
    "        # b contains all known target points\n",
    "        b = pts_t.T.reshape((2 * pts_num, 1))\n",
    "\n",
    "        try:\n",
    "            # Solve the linear equation\n",
    "            theta = np.linalg.lstsq(M, b)[0]\n",
    "\n",
    "            # Form the affine transformation\n",
    "            A = theta[:4].reshape((2, 2))\n",
    "            t = theta[4:]\n",
    "        except np.linalg.linalg.LinAlgError:\n",
    "            # If M is singular matrix, return None\n",
    "            # print(\"Singular matrix.\")\n",
    "            A = None\n",
    "            t = None\n",
    "\n",
    "        return A, t\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# Create instance\n",
    "af = Affine_Transform()\n",
    "\n",
    "# Generate a test case as validation with\n",
    "# a rate of outliers\n",
    "outlier_rate = 0.90\n",
    "A_true, t_true, pts_s, pts_t = af.create_test_case(outlier_rate)\n",
    "\n",
    "# At least 3 corresponding points to\n",
    "# estimate affine transformation\n",
    "K = 3\n",
    "# Randomly select 3 pairs of points to do estimation\n",
    "idx = np.random.randint(0, pts_s.shape[1], (K, 1))\n",
    "\n",
    "A_test, t_test = af.estimate_affine(pts_s[:, idx], pts_t[:, idx])\n",
    "\n",
    "# Display known parameters with estimations\n",
    "# They should be same when outlier_rate equals to 0,\n",
    "# otherwise, they are totally different in some cases\n",
    "#print(A_true, '\\n', t_true)\n",
    "#print(A_test, '\\n', t_test)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Test Class Ransac\n",
    "# The number of iterations in RANSAC\n",
    "ITER_NUM = 2000\n",
    "\n",
    "\n",
    "class Ransac():\n",
    "\n",
    "    def __init__(self, K=3, threshold=1):\n",
    "        ''' __INIT__\n",
    "\n",
    "            Initialize the instance.\n",
    "\n",
    "            Input argements:\n",
    "\n",
    "            - K : the number of corresponding points,\n",
    "            default is 3\n",
    "            - threshold : determing which points are inliers\n",
    "            by comparing residual with it\n",
    "\n",
    "        '''\n",
    "\n",
    "        self.K = K\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def residual_lengths(self, A, t, pts_s, pts_t):\n",
    "        ''' RESIDUAL_LENGTHS\n",
    "\n",
    "            Compute residual length (Euclidean distance) between\n",
    "            estimation and real target points. Estimation are\n",
    "            calculated by the given source point and affine\n",
    "            transformation (A & t).\n",
    "\n",
    "            Input arguments:\n",
    "\n",
    "            - A, t : the estimated affine transformation calculated\n",
    "            by least squares method\n",
    "            - pts_s : key points from source image\n",
    "            - pts_t : key points from target image\n",
    "\n",
    "            Output:\n",
    "\n",
    "            - residual : Euclidean distance between estimated points\n",
    "            and real target points\n",
    "\n",
    "        '''\n",
    "\n",
    "        if not(A is None) and not(t is None):\n",
    "            # Calculate estimated points:\n",
    "            # pts_e = A * pts_s + t\n",
    "            pts_e = np.dot(A, pts_s) + t\n",
    "\n",
    "            # Calculate the residual length between estimated points\n",
    "            # and target points\n",
    "            diff_square = np.power(pts_e - pts_t, 2)\n",
    "            residual = np.sqrt(np.sum(diff_square, axis=0))\n",
    "        else:\n",
    "            residual = None\n",
    "\n",
    "        return residual\n",
    "\n",
    "    def ransac_fit(self, pts_s, pts_t):\n",
    "        ''' RANSAC_FIT\n",
    "\n",
    "            Apply the method of RANSAC to obtain the estimation of\n",
    "            affine transformation and inliers as well.\n",
    "\n",
    "            Input arguments:\n",
    "\n",
    "            - pts_s : key points from source image\n",
    "            - pts_t : key points from target image\n",
    "\n",
    "            Output:\n",
    "\n",
    "            - A, t : estimated affine transformation\n",
    "            - inliers : indices of inliers that will be applied to refine the\n",
    "            affine transformation\n",
    "\n",
    "        '''\n",
    "        #### YOUR CODE START HERE\n",
    "\n",
    "\n",
    "        \n",
    "        #### YOUR CODE ENDS HERE\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# Create instance\n",
    "rs = Ransac(K=3, threshold=1)\n",
    "\n",
    "residual = rs.residual_lengths(A_test, t_test, pts_s, pts_t)\n",
    "\n",
    "# Run RANSAC to estimate affine transformation when\n",
    "# too many outliers in points set\n",
    "A_rsc, t_rsc, inliers = rs.ransac_fit(pts_s, pts_t)\n",
    "\n",
    "# print the number of inliners or point matches\n",
    "print (inliers[0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnf_pn2N-vS7"
   },
   "source": [
    "### Question 3: Detect corners in a given image using Harris Corner Detection Algorithm\n",
    "\n",
    "Find the number of detected corner points in a given image using Harris Corner Detection Algorithm. Note that, Following criterion MUST be satisfied while applying Harris Corner detection Algorithm:\n",
    "\n",
    "(a)  The size of neighbourhood considered for corner detection = 2.\n",
    "(b)  Aperture parameter of Sobel derivative used = 3.\n",
    "(c)  Harris detector free parameter in the equation = 0.04.\n",
    "\n",
    "How many corners are detected?\n",
    "\n",
    "1.   169\n",
    "2.   222\n",
    "3.   264\n",
    "4.   309\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQSynG8c-0y4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Read in the image\n",
    "image = cv2.imread('line.png')\n",
    "\n",
    "# Make a copy of the image\n",
    "image_copy = np.copy(image)\n",
    "\n",
    "# Change color to RGB (from BGR)\n",
    "image_copy = cv2.cvtColor(image_copy, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "###YOUR CODE STARTS HERE\n",
    "\n",
    "## STEP 1:  Convert to grayscale \n",
    "gray = cv2.cvtColor(image_copy, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "## STEP 2: Detect corners \n",
    "dst = cv2.cornerHarris(gray,2,3,0.04)\n",
    "\n",
    "## STEP 3: Dilate corner image to enhance corner points\n",
    "dst = cv2.dilate(dst,None)\n",
    "\n",
    "## STEP 4:set threshold value as 0.1 * (maximum value of dilated corner image obtained from STEP3)\n",
    "#img[dst>0.01*dst.max()]=[0,0,255]\n",
    "\n",
    "## STEP 5: Count numer of detected corner points and draw them on the image\n",
    "s=0\n",
    "for i in range(dst.shape[0]):\n",
    "    for j in range(dst.shape[1]):\n",
    "        if dst[i,j]>0.1*dst.max():\n",
    "            s+=1\n",
    "print(s)\n",
    "\n",
    "### YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXrQtCnwhz5J"
   },
   "source": [
    "# Part-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "FVEgO_bNhLtx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import timeit\n",
    "import unittest\n",
    "\n",
    "## Please DONOT remove these lines. \n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mc4Nl4gCLyiN"
   },
   "source": [
    "### Data Loading and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ECXvC1tUhpPA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "1.6.0\n"
     ]
    }
   ],
   "source": [
    "# check availability of GPU and set the device accordingly\n",
    "#### YOUR CODE STARTS HERE ####\n",
    "device = torch.device(\"cuda\")\n",
    "print(device)\n",
    "print(torch.__version__)\n",
    "#### YOUR CODE ENDS HERE ####\n",
    "\n",
    "# define a transforms for preparing the dataset\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(), # convert the image to a pytorch tensor\n",
    "        transforms.Normalize((0.1307,), (0.3081,)) # normalise the images with mean and std of the dataset\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "1e88bU4vhtdG"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "# Load the MNIST training, test datasets using `torchvision.datasets.MNIST` using the transform defined above\n",
    "#### YOUR CODE STARTS HERE ####\n",
    "train_dataset = torchvision.datasets.MNIST(root = \"./data\", train = True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root = \"./data\", train = False, download=True, transform=transform)\n",
    "\n",
    "# print(train_dataset[0][0])\n",
    "# img = train_dataset[0][0].permute(1,2,0)\n",
    "# plt.imshow(img)\n",
    "# print(len(train_dataset))\n",
    "# print(print(train_dataset[0][1]))\n",
    "# print(len(test_dataset))\n",
    "# print(train_dataset[0][0].size())\n",
    "#### YOUR CODE ENDS HERE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "r2eBGz9OhvD1"
   },
   "outputs": [],
   "source": [
    "# create dataloaders for training and test datasets\n",
    "# use a batch size of 32 and set shuffle=True for the training set\n",
    "#### YOUR CODE STARTS HERE ####\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "#### YOUR CODE ENDS HERE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFfzN-oaL762"
   },
   "source": [
    "*italicised text*### Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "FPQMmDKbhwy9"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        # define a conv layer with output channels as 16, kernel size of 3 and stride of 1\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, stride=1)\n",
    "        # define a conv layer with output channels as 32, kernel size of 3 and stride of 1\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, stride=1)\n",
    "        # define a conv layer with output channels as 64, kernel size of 3 and stride of 1\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, stride=1)\n",
    "        # define a max pooling layer with kernel size 2\n",
    "        self.maxpool = nn.MaxPool2d(2, stride=2)\n",
    "        # define dropout layer with a probability of 0.25\n",
    "        self.dropout1 = nn.Dropout(p=0.25)\n",
    "        # define dropout layer with a probability of 0.5\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        # define a linear(dense) layer with 128 output features\n",
    "        self.fc1 = nn.Linear(in_features=64*11*11, out_features=128)\n",
    "        # define a linear(dense) layer with output features corresponding to the number of classes in the dataset\n",
    "        self.fc2 = nn.Linear(128,10)\n",
    "        self.act = nn.ReLU()\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use the layers defined above in a sequential way (folow the same as the layer definitions above) and \n",
    "        # write the forward pass, after each of conv1, conv2, conv3 and fc1 use a relu activation. \n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        x = self.act(self.conv1(x))\n",
    "        x = self.act(self.conv2(x))\n",
    "        x = self.act(self.conv3(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        #print(x.size())\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_8IZKqGiAhM"
   },
   "source": [
    "### Sanity Check\n",
    "Make sure all the tests below pass without any errors, before you proceed with the training part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "KAiwx-TSyPK6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.029s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=3 errors=0 failures=0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestImplementations(unittest.TestCase):\n",
    "    \n",
    "    # Dataloading tests\n",
    "    def test_dataset(self):\n",
    "        self.dataset_classes = ['0 - zero',\n",
    "                                '1 - one',\n",
    "                                '2 - two',\n",
    "                                '3 - three',\n",
    "                                '4 - four',\n",
    "                                '5 - five',\n",
    "                                '6 - six',\n",
    "                                '7 - seven',\n",
    "                                '8 - eight',\n",
    "                                '9 - nine']\n",
    "        self.assertTrue(train_dataset.classes == self.dataset_classes)\n",
    "        self.assertTrue(train_dataset.train == True)\n",
    "    \n",
    "    def test_dataloader(self):        \n",
    "        self.assertTrue(train_dataloader.batch_size == 32)\n",
    "        self.assertTrue(test_dataloader.batch_size == 32)      \n",
    "         \n",
    "    def test_total_parameters(self):\n",
    "        model = Net().to(device)\n",
    "        self.assertTrue(sum(p.numel() for p in model.parameters()) == 1015946)\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromModule(TestImplementations())\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GkrWn_dfVyo"
   },
   "source": [
    "### Training and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "IIdgIfkAGXi9"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "      #### YOUR CODE STARTS HERE ####\n",
    "        # send the image, target to the device\n",
    "        data = data.to(device)\n",
    "        #print(data.shape)\n",
    "        target = target.to(device)\n",
    "        #print(target.shape)\n",
    "        # flush out the gradients stored in optimizer\n",
    "        model.zero_grad()\n",
    "        # pass the image to the model and assign the output to variable named output\n",
    "        output = model(data)\n",
    "        # calculate the loss (use nll_loss in pytorch)\n",
    "        loss = nn.NLLLoss()(output, target)\n",
    "        # do a backward pass\n",
    "        loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "      #### YOUR CODE ENDS HERE ####\n",
    "        if batch_idx % 20 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "jWEpNBtdHVWr"
   },
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "          ### YOUR CODE STARTS HERE ####\n",
    "            # send the image, target to the device\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            # pass the image to the model and assign the output to variable named output\n",
    "            output = model(data)\n",
    "            test_loss += nn.NLLLoss()(output, target).item()# sum up batch loss\n",
    "          #### YOUR CODE ENDS HERE ####\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m02MLg2mxRwB"
   },
   "source": [
    "### Question 4\n",
    "\n",
    "Run the code cell below and report the final test accuracy (If you are not getting the exact number shown in options, please report the closest number).\n",
    "1. 64%\n",
    "2. 79%\n",
    "3. 97%\n",
    "4. 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "MRebLHiriRsu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.299003\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 0.694414\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.472681\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.422518\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.559182\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.307822\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.373730\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.181192\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.244105\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.404234\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.525527\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.280086\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.015073\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.296812\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.143300\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.891806\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.707764\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.150127\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.285209\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.370113\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.186115\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.101027\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-d6fa70b333b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mstop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-043dd7cdd983>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m#print(target.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m# flush out the gradients stored in optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;31m# pass the image to the model and assign the output to variable named output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\carnd-gpu1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mzero_grad\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1319\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mshare_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "\n",
    "## Define Adam Optimiser with a learning rate of 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "for epoch in range(1, 11):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)\n",
    "stop = timeit.default_timer()\n",
    "print('Total time taken: {} seconds'.format(int(stop - start)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TV3DSN3v0NBN"
   },
   "source": [
    "### Question 5\n",
    "\n",
    "Modify the network to replace ReLU activations with Sigmoid and report the final test accuracy by running the cell below. (If you are not getting the exact number shown in options, please report the closest number). \n",
    "\n",
    "1. 95%\n",
    "2. 54%\n",
    "3. 20%\n",
    "4. 9%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-2mgA4T0KeU"
   },
   "outputs": [],
   "source": [
    "class NetSigmoid(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetSigmoid, self).__init__()\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        # define a conv layer with output channels as 16, kernel size of 3 and stride of 1\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, stride=1)\n",
    "        # define a conv layer with output channels as 32, kernel size of 3 and stride of 1\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, stride=1)\n",
    "        # define a conv layer with output channels as 64, kernel size of 3 and stride of 1\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, stride=1)\n",
    "        # define a max pooling layer with kernel size 2\n",
    "        self.maxpool = nn.MaxPool2d(2, stride=2)\n",
    "        # define dropout layer with a probability of 0.25\n",
    "        self.dropout1 = nn.Dropout(p=0.25)\n",
    "        # define dropout layer with a probability of 0.5\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        # define a linear(dense) layer with 128 output features\n",
    "        self.fc1 = nn.Linear(in_features=64*11*11, out_features=128)\n",
    "        # define a linear(dense) layer with output features corresponding to the number of classes in the dataset\n",
    "        self.fc2 = nn.Linear(128,10)\n",
    "        self.act = nn.Sigmoid()\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use the layers defined above in a sequential way (folow the same as the layer definitions above) and \n",
    "        # write the forward pass, after each of conv1, conv2, conv3 and fc1 use a relu activation. \n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        x = self.act(self.conv1(x))\n",
    "        x = self.act(self.conv2(x))\n",
    "        x = self.act(self.conv3(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        #print(x.size())\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0oZc3cTxJ90"
   },
   "outputs": [],
   "source": [
    "model = NetSigmoid().to(device)\n",
    "\n",
    "## Define Adam Optimiser with a learning rate of 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Total time taken: {} seconds'.format(int(stop - start)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lurbs-F35AqX"
   },
   "source": [
    "### Question 6\n",
    "\n",
    "Train the network defined in Question-1 with Adagrad optimizer (with the same learning rate). Report the final test accuracy by running the cell below. (If you are not getting the exact number shown in options, please report the closest number). \n",
    "\n",
    "1. 80%\n",
    "2. 99%\n",
    "3. 92%\n",
    "4. 85%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cuddf3Tb5Oi6"
   },
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "\n",
    "## Define Adam Optimiser with a learning rate of 0.01\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr = 0.01)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Total time taken: {} seconds'.format(int(stop - start)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qXfhHoS3siEi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m34pGNerYmld"
   },
   "source": [
    "### Question 7\n",
    "\n",
    "Initialize the `Conv2d` layers in the network defined in Question-1 `(Net)` with all ones (both weights and bias). Train the network with Adam optimizer and report the final test accuracy by running the cell below. (If you are not getting the exact number shown in options, please report the closest number). \n",
    "\n",
    "1. 11%\n",
    "2. 18%\n",
    "3. 97%\n",
    "4. 6%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GsHOjQA_Ylzj"
   },
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "  #### YOUR CODE STARTS HERE ####\n",
    "  if isinstance(m, nn.Conv2d):\n",
    "        nn.init.ones_(m.weight.data)\n",
    "        nn.init.ones_(m.bias.data)\n",
    "  #### YOUR CODE ENDS HERE ####\n",
    "  \n",
    "\n",
    "model.apply(init_weights)  \n",
    "## Define Adam Optimiser with a learning rate of 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "for epoch in range(1, 5):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Total time taken: {} seconds'.format(int(stop - start)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHmX6YO_ay9n"
   },
   "source": [
    "### Question 8\n",
    "\n",
    "Initialize the network defined in Question-1 `(Net)` with Xavier's initialization ([torch.nn.init.xavier_normal](https://pytorch.org/docs/stable/nn.init.html))(for bias use zero). Train the network with Adam optimizer and report the final test accuracy by running the cell below. (If you are not getting the exact number shown in options, please report the closest number). \n",
    "\n",
    "\n",
    "1. 88%\n",
    "2. 74%\n",
    "3. 97%\n",
    "4. 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ibfj8A9xZsEw"
   },
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "  #### YOUR CODE STARTS HERE ####\n",
    "  if isinstance(m, nn.Conv2d):\n",
    "    nn.init.xavier_normal_(m.weight.data)\n",
    "    nn.init.zeros_(m.bias.data)\n",
    "  #### YOUR CODE ENDS HERE #### \n",
    "\n",
    "model.apply(init_weights)  \n",
    "## Define Adam Optimiser with a learning rate of 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "for epoch in range(1, 5):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Total time taken: {} seconds'.format(int(stop - start)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3OqBlnPe4LR"
   },
   "source": [
    "### Question 9\n",
    "\n",
    "Add three batch-norm layers to the network defined in `Question-1` and report the final test accuracy by running the cell below. \n",
    "\n",
    "1. 92%\n",
    "2. 89%\n",
    "3. 98%\n",
    "4. 74%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j2sNY6Tge1EO"
   },
   "outputs": [],
   "source": [
    "class NetBatchNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetBatchNorm, self).__init__()\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        # define a conv layer with output channels as 16, kernel size of 3 and stride of 1\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, stride=1)\n",
    "        # define a batchnorm layer (2d) with 16 features\n",
    "        self.norm1 = nn.BatchNorm2d(16)\n",
    "        # define a conv layer with output channels as 32, kernel size of 3 and stride of 1\n",
    "        self.conv2 = nn.Conv2d(16,32,3, stride=1)\n",
    "        # define a batchnorm layer (2d) with 32 features\n",
    "        self.norm2 = nn.BatchNorm2d(32)\n",
    "        # define a conv layer with output channels as 64, kernel size of 3 and stride of 1\n",
    "        self.conv3 = nn.Conv2d(32,64,3, stride = 1)\n",
    "        # define a batchnorm layer (2d) with 64 features\n",
    "        self.norm3 = nn.BatchNorm2d(64)\n",
    "        self.maxpool = nn.MaxPool2d(2, stride=2)\n",
    "        # define dropout layer with a probability of 0.25\n",
    "        self.drop1 = nn.Dropout(p=0.25)\n",
    "        # define dropout layer with a probability of 0.5\n",
    "        self.drop2 = nn.Dropout(p=0.5)\n",
    "        # define a linear(dense) layer with 128 output features\n",
    "        self.fc1 = nn.Linear(64*11*11, 128)\n",
    "        # define a linear(dense) layer with output features corresponding to the number of classes in the dataset\n",
    "        self.fc2 = nn.Linear(128,10)\n",
    "        self.act = nn.ReLU()\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "\n",
    "    def forward(self, x):\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        x = self.conv1(x)\n",
    "        x = self.act(self.norm1(x))\n",
    "        x = self.conv2(x)\n",
    "        x = self.act(self.norm2(x))\n",
    "        x = self.conv3(x)\n",
    "        x = self.act(self.norm3(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.drop2(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZ_-nFokSbSh"
   },
   "outputs": [],
   "source": [
    "model = NetBatchNorm().to(device)\n",
    "# If you implemented the batchnorm layers correctly below line should return true\n",
    "sum(p.numel() for p in model.parameters()) == 1016170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3hmkY3j-hFWC"
   },
   "outputs": [],
   "source": [
    "## Define Adam Optimiser with a learning rate of 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "for epoch in range(1, 11):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)\n",
    "stop = timeit.default_timer()\n",
    "print('Total time taken: {} seconds'.format(int(stop - start)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLjR0rM3nFnq"
   },
   "source": [
    "# Part-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxogDSnVqcjO"
   },
   "source": [
    "This section is un-graded and purely for practice. \n",
    "\n",
    "Main focus of this part is to help you flex the deep learning muscles built in the above part. You should build a network on the [SVHN dataset](http://ufldl.stanford.edu/housenumbers/). This dataset is similar to MNIST but unlike MNIST, the images are colored and more complex. \n",
    "\n",
    "As of writing this, the state-of-the-art(SoTA) performance on this dataset is 98.98%. You can try to start with the simple network we defined above for the MNSIT dataset(with some modification for dealing with different sized colored images unlike MNIST). But to achive the SoTA performance you need to do a lot of hackery. These are list of few things, we would encourage you to try: \n",
    "\n",
    "- Use data augmentation wisely. Read and understand how to perform the augmentations listed below. \n",
    "    * RandomFlips, Color Jittering\n",
    "    * Cutout, Cutmix\n",
    "    * Mixup\n",
    "    * Auto-augment\n",
    "\n",
    "- Try to increase the image size using standard image interpolation techniques. Try using tricks like Progressive resizing of images and see if it helps. \n",
    "\n",
    "- After certain number of layers, adding more layer might not be of much help, run experiments on SVHN and see if you observe this. \n",
    "\n",
    "- To understand the difficulties in training deeper networks read this paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "- To improve the performance on SVHN, try using architectures like [ResNet](https://arxiv.org/abs/1512.03385), [DesnseNet](https://arxiv.org/abs/1608.06993) or [EfficientNet](https://arxiv.org/abs/1905.11946). Most of these architectures are available by default in PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73257, 1)\n",
      "73257\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_file = \"C:/Users/manas/Downloads/train_32x32\"\n",
    "train_data = scipy.io.loadmat(train_file)\n",
    "X_data = train_data['X']\n",
    "y_data = train_data['y'] #done with importing the training data \n",
    "\n",
    "\n",
    "plt.imshow(X_data[:,:,:,15]) \n",
    "print(y_data.shape)\n",
    "print(X_data.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import timeit\n",
    "import unittest\n",
    "\n",
    "## Please DONOT remove these lines. \n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVHNDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, train_file):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            train_file (string): Path to the training file with the labels\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.train_data = scipy.io.loadmat(train_file)\n",
    "        self.X_data = self.train_data[\"X\"] # slicing the 4D matrix that contains data (H,W,C,B)\n",
    "        self.y_data = self.train_data[\"y\"] # slicing the labels \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X_data.shape[3] #length of the training set\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "\n",
    "        if torch.is_tensor(id):\n",
    "            id = id.tolist()\n",
    "\n",
    "        # a sample of the dataset will contain the image and the label as a tuple\n",
    "\n",
    "        image = self.X_data[:,:,:, id]\n",
    "        label = self.y_data[id]\n",
    "\n",
    "        return (image, label)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n",
      "2\n",
      "The label is [2]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "svhn_dataset = SVHNDataset(train_file = \"C:/Users/manas/Downloads/train_32x32\")\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "image = svhn_dataset[2][0] # getting the second image from the dataset\n",
    "\n",
    "plt.imshow(image) #plotting the image \n",
    "print(image.shape)\n",
    "print(svhn_dataset[2][1][0])\n",
    "\n",
    "print(\"The label is {}\".format(svhn_dataset[2][1])) #printing the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def compute_mean_std(svhn_dataset):\n",
    "    \"\"\"compute the mean and std of svhn dataset\n",
    "    Args:\n",
    "        svhn_training_dataset or cifar100_test_dataset\n",
    "        witch derived from class torch.utils.data\n",
    "    \n",
    "    Returns:\n",
    "        a tuple contains mean, std value of entire dataset\n",
    "    \"\"\"\n",
    "\n",
    "    data_r = numpy.dstack([svhn_dataset[i][0][0, :, :] for i in range(len(svhn_dataset))])\n",
    "    data_g = numpy.dstack([svhn_dataset[i][0][1, :, :] for i in range(len(svhn_dataset))])\n",
    "    data_b = numpy.dstack([svhn_dataset[i][0][2, :, :] for i in range(len(svhn_dataset))])\n",
    "    mean = numpy.mean(data_r), numpy.mean(data_g), numpy.mean(data_b)\n",
    "    std = numpy.std(data_r), numpy.std(data_g), numpy.std(data_b)\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "# mean, std = compute_mean_std(svhn_dataset)\n",
    "# print(mean)\n",
    "# print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torchvision\n",
    "import scipy\n",
    "\"\"\"\n",
    "    Applying various transformations/preprocessing techniques on the dataset \n",
    "\"\"\"\n",
    "\n",
    "transform_svhn_train = transforms.Compose([\n",
    "        transforms.Resize(64, interpolation = 2), #increasing the image size using bilinear interpolation\n",
    "        transforms.ToTensor(), # convert the image to a pytorch tensor\n",
    "        #transforms.RandomHorizontalFlip(), #random horizontal flipping with a probability of 0.5\n",
    "        transforms.ColorJitter(brightness=0.4), # random color jittering with saturation and brightness\n",
    "        #transforms.Normalize((0.437, 0.443, 0.472), (0.198, 0.201, 0.197)),\n",
    "        transforms.Normalize((0.438, 0.444, 0.473), (0.195, 0.198, 0.195)),\n",
    "        \n",
    "        ])\n",
    "\n",
    "transform_svhn_test = transforms.Compose([\n",
    "        transforms.Resize(64, interpolation = 2), #increasing the image size using bilinear interpolation\n",
    "        transforms.ToTensor(), # convert the image to a pytorch tensor\n",
    "        transforms.Normalize((0.438, 0.444, 0.473), (0.195, 0.198, 0.195)),\n",
    "        \n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data_svhn\\train_32x32.mat\n",
      "Using downloaded and verified file: ./data_svhn\\test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "train_dataset = torchvision.datasets.SVHN(root = \"./data_svhn\", split = \"train\", download = True, transform = transform_svhn_train)\n",
    "test_dataset = torchvision.datasets.SVHN(root = \"./data_svhn\", split = \"test\", download = True, transform = transform_svhn_test)\n",
    "\n",
    "# print(train_dataset[0][0])\n",
    "# image = train_dataset[0][0].permute(1,2,0)\n",
    "# image -= image.min() \n",
    "# image /= image.max()\n",
    "# print(image)\n",
    "# image = 255*image\n",
    "# plt.imshow(image)\n",
    "# print(image)\n",
    "# mean, std = compute_mean_std(train_dataset)\n",
    "# print(mean)\n",
    "# print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000002E3C0147D08>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Creating dataloaders for train and test datasets\n",
    "        Args:\n",
    "        batch_size = 32\n",
    "        shuffle = True for training set\n",
    "\"\"\"\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size = 32, shuffle = True)\n",
    "\n",
    "print(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "\n",
    "def svhn_train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_id, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        model.zero_grad() #calling the zero_grad function to flush out the gradients \n",
    "        output = model(data)\n",
    "        loss = nn.NLLLoss()(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_id%1000 == 0:\n",
    "            print('Train epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_id*len(data), \n",
    "                        len(train_loader.dataset), 100. * batch_id/len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svhn_test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            target = torch.tensor(target, dtype = torch.long, device = device)\n",
    "            #print(target)\n",
    "            output = model(data)\n",
    "            test_loss += nn.NLLLoss()(output, target).item() #sum up batch loss\n",
    "            pred = output.argmax(dim =1, keepdim=True) #this is the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    return test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SVHNNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SVHNNet, self).__init__()\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, stride=1)\n",
    "        self.norm1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, stride=1)\n",
    "        self.norm2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, stride=1)\n",
    "        self.norm3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 128, 3, stride=1)\n",
    "        self.norm4 = nn.BatchNorm2d(128)\n",
    "        self.maxpool = nn.MaxPool2d(2, stride=2)\n",
    "        self.conv5 = nn.Conv2d(128, 256, 3, stride=1)\n",
    "        self.norm5 = nn.BatchNorm2d(256)\n",
    "        self.conv6 = nn.Conv2d(256, 256, 3, stride=1)\n",
    "        self.norm6 = nn.BatchNorm2d(256)\n",
    "        self.conv7 = nn.Conv2d(256, 256, 3, stride=1)\n",
    "        self.norm7 = nn.BatchNorm2d(256)\n",
    "        self.conv8 = nn.Conv2d(256, 256, 3, stride=1)\n",
    "        self.norm8 = nn.BatchNorm2d(256)\n",
    "        self.dropout1 = nn.Dropout(p=0.25)\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        self.fc1 = nn.Linear(in_features=256*10*10, out_features=1000)\n",
    "        self.fc2 = nn.Linear(1000,400)\n",
    "        self.fc3 = nn.Linear(400,100)\n",
    "        self.fc4 = nn.Linear(100,10)\n",
    "        self.act = nn.ReLU()\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use the layers defined above in a sequential way (folow the same as the layer definitions above) and \n",
    "        # write the forward pass, after each of conv1, conv2, conv3 and fc1 use a relu activation. \n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        x = self.conv1(x)\n",
    "        x = self.act(self.norm1(x))\n",
    "        x = self.conv2(x)\n",
    "        x = self.act(self.norm2(x))\n",
    "        x = self.conv3(x)\n",
    "        x = self.act(self.norm3(x))\n",
    "        x = self.conv4(x)\n",
    "        x = self.act(self.norm4(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.act(self.norm5(x))\n",
    "        x = self.conv6(x)\n",
    "        x = self.act(self.norm6(x))\n",
    "        x = self.conv7(x)\n",
    "        x = self.act(self.norm7(x))\n",
    "        x = self.conv8(x)\n",
    "        x = self.act(self.norm8(x))\n",
    "        x = self.maxpool(x)\n",
    "        #x = self.dropout1(x)\n",
    "        #x = self.dropout2(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        #print(x.size())\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.act(self.fc2(x))\n",
    "        x = self.act(self.fc3(x))\n",
    "        x = self.act(self.fc4(x))\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch: 1 [0/73257 (0%)]\tLoss: 2.904352\n",
      "Train epoch: 1 [32000/73257 (44%)]\tLoss: 0.068181\n",
      "Train epoch: 1 [64000/73257 (87%)]\tLoss: 0.020934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manas\\anaconda3\\envs\\carnd-gpu1\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0078, Accuracy: 24635/26032 (95%)\n",
      "\n",
      "Train epoch: 2 [0/73257 (0%)]\tLoss: 0.013225\n",
      "Train epoch: 2 [32000/73257 (44%)]\tLoss: 0.020332\n",
      "Train epoch: 2 [64000/73257 (87%)]\tLoss: 0.024003\n",
      "\n",
      "Test set: Average loss: 0.0077, Accuracy: 24640/26032 (95%)\n",
      "\n",
      "Train epoch: 3 [0/73257 (0%)]\tLoss: 0.007965\n",
      "Train epoch: 3 [32000/73257 (44%)]\tLoss: 0.061846\n",
      "Train epoch: 3 [64000/73257 (87%)]\tLoss: 0.062187\n",
      "\n",
      "Test set: Average loss: 0.0087, Accuracy: 24417/26032 (94%)\n",
      "\n",
      "Train epoch: 4 [0/73257 (0%)]\tLoss: 0.006252\n",
      "Train epoch: 4 [32000/73257 (44%)]\tLoss: 0.000960\n",
      "Train epoch: 4 [64000/73257 (87%)]\tLoss: 0.022109\n",
      "\n",
      "Test set: Average loss: 0.0081, Accuracy: 24554/26032 (94%)\n",
      "\n",
      "Train epoch: 5 [0/73257 (0%)]\tLoss: 0.014100\n",
      "Train epoch: 5 [32000/73257 (44%)]\tLoss: 0.069190\n",
      "Train epoch: 5 [64000/73257 (87%)]\tLoss: 0.025565\n",
      "\n",
      "Test set: Average loss: 0.0082, Accuracy: 24634/26032 (95%)\n",
      "\n",
      "Train epoch: 6 [0/73257 (0%)]\tLoss: 0.004096\n",
      "Train epoch: 6 [32000/73257 (44%)]\tLoss: 0.017827\n",
      "Train epoch: 6 [64000/73257 (87%)]\tLoss: 0.020834\n",
      "\n",
      "Test set: Average loss: 0.0092, Accuracy: 24488/26032 (94%)\n",
      "\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Train epoch: 7 [0/73257 (0%)]\tLoss: 0.001519\n",
      "Train epoch: 7 [32000/73257 (44%)]\tLoss: 0.002544\n",
      "Train epoch: 7 [64000/73257 (87%)]\tLoss: 0.000477\n",
      "\n",
      "Test set: Average loss: 0.0081, Accuracy: 24702/26032 (95%)\n",
      "\n",
      "Train epoch: 8 [0/73257 (0%)]\tLoss: 0.000352\n",
      "Train epoch: 8 [32000/73257 (44%)]\tLoss: 0.004614\n",
      "Train epoch: 8 [64000/73257 (87%)]\tLoss: 0.000196\n",
      "\n",
      "Test set: Average loss: 0.0081, Accuracy: 24765/26032 (95%)\n",
      "\n",
      "Train epoch: 9 [0/73257 (0%)]\tLoss: 0.000323\n",
      "Train epoch: 9 [32000/73257 (44%)]\tLoss: 0.000418\n",
      "Train epoch: 9 [64000/73257 (87%)]\tLoss: 0.083968\n",
      "\n",
      "Test set: Average loss: 0.0085, Accuracy: 24752/26032 (95%)\n",
      "\n",
      "Train epoch: 10 [0/73257 (0%)]\tLoss: 0.000563\n",
      "Train epoch: 10 [32000/73257 (44%)]\tLoss: 0.000168\n",
      "Train epoch: 10 [64000/73257 (87%)]\tLoss: 0.000372\n",
      "\n",
      "Test set: Average loss: 0.0091, Accuracy: 24740/26032 (95%)\n",
      "\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Train epoch: 11 [0/73257 (0%)]\tLoss: 0.000226\n",
      "Train epoch: 11 [32000/73257 (44%)]\tLoss: 0.007632\n",
      "Train epoch: 11 [64000/73257 (87%)]\tLoss: 0.001473\n",
      "\n",
      "Test set: Average loss: 0.0089, Accuracy: 24761/26032 (95%)\n",
      "\n",
      "Train epoch: 12 [0/73257 (0%)]\tLoss: 0.001817\n",
      "Train epoch: 12 [32000/73257 (44%)]\tLoss: 0.000175\n",
      "Train epoch: 12 [64000/73257 (87%)]\tLoss: 0.002524\n",
      "\n",
      "Test set: Average loss: 0.0088, Accuracy: 24782/26032 (95%)\n",
      "\n",
      "Train epoch: 13 [0/73257 (0%)]\tLoss: 0.005863\n",
      "Train epoch: 13 [32000/73257 (44%)]\tLoss: 0.000125\n",
      "Train epoch: 13 [64000/73257 (87%)]\tLoss: 0.000175\n",
      "\n",
      "Test set: Average loss: 0.0088, Accuracy: 24797/26032 (95%)\n",
      "\n",
      "Train epoch: 14 [0/73257 (0%)]\tLoss: 0.000310\n",
      "Train epoch: 14 [32000/73257 (44%)]\tLoss: 0.020818\n",
      "Train epoch: 14 [64000/73257 (87%)]\tLoss: 0.000480\n",
      "\n",
      "Test set: Average loss: 0.0088, Accuracy: 24786/26032 (95%)\n",
      "\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Train epoch: 15 [0/73257 (0%)]\tLoss: 0.000238\n",
      "Train epoch: 15 [32000/73257 (44%)]\tLoss: 0.000119\n",
      "Train epoch: 15 [64000/73257 (87%)]\tLoss: 0.001468\n",
      "\n",
      "Test set: Average loss: 0.0089, Accuracy: 24780/26032 (95%)\n",
      "\n",
      "Total time taken: 1030 seconds\n"
     ]
    }
   ],
   "source": [
    "#model = SVHNNet().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "#model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "    \n",
    "# model.fc = nn.Linear(512, 10)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Dropout(0.25),\n",
    "    nn.Linear(512, 10),\n",
    "    nn.LogSoftmax()\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.1, patience = 3, verbose=True)\n",
    "filepath = \"model.pth\"\n",
    "\n",
    "start = timeit.default_timer()\n",
    "for epoch in range(1, 16):\n",
    "    svhn_train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test_loss = svhn_test(model, device, test_dataloader)\n",
    "    scheduler.step(test_loss)\n",
    "\n",
    "torch.save(model.state_dict(), filepath)\n",
    "stop = timeit.default_timer()\n",
    "print('Total time taken: {} seconds'.format(int(stop - start)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DL4V_Assignment_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "carnd-gpu1",
   "language": "python",
   "name": "carnd-gpu1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ou0msMUmgx6m"
   },
   "source": [
    "#### **Welcome to Assignment 3 on Deep Learning for Computer Vision.**\n",
    "<!-- This assignment consists of three parts. Part-1 is based on the content you learned in Week-3 of course and Part-2 is based on the content you learned in Week-4 of the course. Part-3 is **un-graded** and mainly designed to help you flex the Deep Learning muscles grown in Part-2. \n",
    "\n",
    "Unlike the first two parts, you'll have to implement everything from scratch in Part-3. If you find answers to questions in Part-3, feel free to head out to the forums and discuss them with your classmates! -->\n",
    "\n",
    "#### **Instructions**\n",
    "1. Use Python 3.x to run this notebook\n",
    "2. Write your code only in between the lines 'YOUR CODE STARTS HERE' and 'YOUR CODE ENDS HERE'.\n",
    "you should not change anything else in the code cells, if you do, the answers you are supposed to get at the end of this assignment might be wrong.\n",
    "3. Read documentation of each function carefully.\n",
    "4. All the Best!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTuteiZLlJcy"
   },
   "source": [
    "\n",
    "### Part-1: Resnet-18 from scratch\n",
    "\n",
    "In this question, you'll have to code Resnet-18 from scratch (we have provided a lot of starter code), this'll help you get a hold on how to code an architecture with skip connections and blocks of layers.\n",
    "\n",
    "It's suggested you first briefly understand how the Resnet architecture is defined originally before you start with this question. We do take inspiration from the original Pytorch implementation, but if you try peeking into the original source code in the library, it'll confuse you more than helping!\n",
    "\n",
    "**Sidenote:** As this assignment is mainly focused on learning things, we train the models only for a small number of epochs and don't focus on hyper-parameter tuning. When you start using deep learning in real-world applications and competitions, hyper-parameter tuning plays a decent role!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GM0uht4kcYVs"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import timeit\n",
    "import unittest\n",
    "\n",
    "## Please DONOT remove these lines. \n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uhNHc_cjmvoo"
   },
   "outputs": [],
   "source": [
    "# check availability of GPU and set the device accordingly\n",
    "device = torch.device(\"cuda\")\n",
    "#print(device)\n",
    "\n",
    "# define a set of transforms for preparing the dataset\n",
    "transform_train = transforms.Compose([\n",
    "        # use random crop with image size fo 32 and padding of 8 \n",
    "        # flip the image horizontally (use pytorch random horizontal flip)\n",
    "        # convert the image to a pytorch tensor\n",
    "        # normalise the images with mean and std of the dataset \n",
    "        # mean: (0.4914, 0.4822, 0.4465) std: (0.2023, 0.1994, 0.2010)\n",
    "        transforms.RandomCrop(32, padding=8),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                              std=[0.2023, 0.1994, 0.2010])])\n",
    "\n",
    "\n",
    "# define transforms for the test data: Should they be same as the one used for train? \n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                         std=[0.2023, 0.1994, 0.2010]) #sequence of means and stds for each channel\n",
    "])\n",
    "        \n",
    "    \n",
    "        # convert the image to a pytorch tensor\n",
    "        # normalise the images with mean and std of the dataset \n",
    "        # mean: (0.4914, 0.4822, 0.4465) std: (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "use_cuda = True # if you have acess to a GPU, enable it to speed the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cQciUzi2oF5Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "50000\n",
      "10000\n",
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Load the CIFAR-10 training, test datasets using `torchvision.datasets.CIFAR10`\n",
    "#### YOUR CODE STARTS HERE ####\n",
    "train_dataset = torchvision.datasets.CIFAR10(root = \"./data\", train = True, download=True, transform=transform_train)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root = \"./data\", train = False, download=True, transform=transform_test)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(train_dataset[0][0].size())\n",
    "#### YOUR CODE ENDS HERE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "A1Za_R3Yrie2"
   },
   "outputs": [],
   "source": [
    "# create dataloaders for training and test datasets\n",
    "# use a batch size of 32 and set shuffle=True for the training set\n",
    "#### YOUR CODE STARTS HERE ####\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "#### YOUR CODE ENDS HERE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gCifYwfbT9Ic"
   },
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1, groups=1):\n",
    "    # define a convolutional layer with a kernel size of 3x3\n",
    "    # use stride, groups values passed to the function along with a padding of 1 and dilation of 1\n",
    "    # set bias to False\n",
    "    #### YOUR CODE STARTS HERE ####\n",
    "    layer = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, dilation=1, bias = False)\n",
    "    #### YOUR CODE ENDS HERE ####\n",
    "    return layer\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    # define a convolutional layer with a kernel size of 1x1\n",
    "    # use stride value passed to the function\n",
    "    # set bias to False\n",
    "    # leave all other parameters to default values\n",
    "    #### YOUR CODE STARTS HERE ####\n",
    "    layer = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "    #### YOUR CODE ENDS HERE ####\n",
    "    return layer\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        # define batch-norm layer to for easy use (you don't have to call it here)\n",
    "        norm_layer = nn.BatchNorm2d\n",
    "        # define a 3x3 convolution layer with inplanes as in-channels and planes and out_channels, use the passed value of stride\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride, groups)\n",
    "        # define a batchnorm layer (use the norm_layer defined above)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        # define a relu layer with inplace set to True\n",
    "        self.relu = nn.ReLU(inplace=True) # modifies the input directly without allocating any output\n",
    "        # define a 3x3 convolution layer with inplanes as in-channels and planes and out_channels\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        # define a batchnorm layer (use the norm_layer defined above)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        # make a copy of nput (for using them in skip connections)\n",
    "        identity = x\n",
    "\n",
    "        # pass the input through, conv1->bn1->relu->conv2->bn2\n",
    "        x1 = self.conv1(x)\n",
    "        x1 = self.bn1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x1 = self.conv2(x1)\n",
    "        x1 = self.bn2(x1)\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        # add the skip connection\n",
    "        out = x1 + identity\n",
    "        out = self.relu(out)\n",
    "        # use a relu activation on `out`\n",
    "\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Sr00CBjlfqsp"
   },
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "  # first start with make_layer method followed by __init__, forward methods\n",
    "    def __init__(self, block, num_classes=10, groups=1):\n",
    "        super(ResNet18, self).__init__()\n",
    "        \n",
    "        # define batch-norm layer to for easy use (you don't have to call it here)\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        norm_layer = nn.BatchNorm2d\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "        self._norm_layer = norm_layer\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "\n",
    "        self.groups = groups\n",
    "        self.base_width = 64\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        # define a conv layer with number of image channels as in-channels and inplanes ans out-channles,\n",
    "        # use a kernel size of 7, stride of 2, padding of 3 and set bias to False \n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, 7, stride=2, padding=3, bias=False)\n",
    "        # define a batchnorm layer (use the norm_layer defined above)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        # define a relu layer with inplace set to True\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # define a maxpool layer with kernel size of 3, stride of 2, padding of 1\n",
    "        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "        # complete the make layer method below and use it with the block value passed to init\n",
    "        # with 64 planes and 2 blocks\n",
    "        self.layer1 = self._make_layer(block, 64, 2, stride =2)\n",
    "        # use  make layer method to define a second set of layers with the block value passed to init\n",
    "        # with 128 planes and 2 blocks and a stride value of 2\n",
    "        self.layer2 = self._make_layer(block, 128, 2, stride=2)\n",
    "        # use  make layer method to define a second set of layers with the block value passed to init\n",
    "        # with 256 planes and 2 blocks and a stride value of 2\n",
    "        self.layer3 = self._make_layer(block, 256, 2, stride=2)\n",
    "        # use  make layer method to define a second set of layers with the block value passed to init\n",
    "        # with 512 planes and 2 blocks and a stride value of 2\n",
    "        self.layer4 = self._make_layer(block, 512, 2, stride=2)\n",
    "        # define  adaptive avergae pooling layer with output size (1, 1)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        #### YOUR CODE STARTS HERE ####        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # initialise the weights with kaiming normal, set mode to fan out and \n",
    "                # non_linearity to the activation function you used above\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                # initialise weights with a value of 1 and bias with a value of 0\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "         #### YOUR CODE ENDS HERE ####\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            #### YOUR CODE STARTS HERE ####\n",
    "            # append the blocks to layers, leave stride and downsample to default values\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width))\n",
    "            #### YOUR CODE ENDS HERE ####\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        # complete the forward pass\n",
    "        # order of layers: conv1->bn1->relu->maxpool->layer1->layer2->layer3->layer4->avgpool->fc\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LmTBdw1NoNPt"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(model, device, train_loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "      #### YOUR CODE STARTS HERE ####\n",
    "        # send the image, target to the device\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        # flush out the gradients stored in optimizer\n",
    "        model.zero_grad()\n",
    "        # pass the image to the model and assign the output to variable named output\n",
    "        output = model(data)\n",
    "        # calculate the loss (use cross entropy in pytorch)\n",
    "        loss = nn.CrossEntropyLoss()(output, target)\n",
    "        # do a backward pass\n",
    "        loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "      #### YOUR CODE ENDS HERE ####\n",
    "        if batch_idx % 20 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JDSnXB7HpuyN"
   },
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "          ### YOUR CODE STARTS HERE ####\n",
    "            # send the image, target to the device\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            # pass the image to the model and assign the output to variable named output\n",
    "            output = model(data)\n",
    "            test_loss += nn.CrossEntropyLoss()(output, target).item()\n",
    "          #### YOUR CODE ENDS HERE ####\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hyoPZ87tMzp-"
   },
   "outputs": [],
   "source": [
    "model = ResNet18(BasicBlock, num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "## Define Adam Optimiser with a learning rate of 0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "for epoch in range(1, 11):\n",
    "    train(model, device, train_dataloader, criterion, optimizer, epoch)\n",
    "    test(model, device, test_dataloader, criterion)\n",
    "stop = timeit.default_timer()\n",
    "print('Total time taken: {} seconds'.format(int(stop - start)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qs-uD1BMb7Yl"
   },
   "source": [
    "#### Question-1\n",
    "\n",
    "Report the final test accuracy displayed above (If you are not getting the exact number shown in options, please report the closest number).\n",
    "1. 94%\n",
    "2. 76%\n",
    "3. 48%\n",
    "4. 85%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8Y26Vp19Zka"
   },
   "source": [
    "### Part-2: Transfer Learning - ResNet50\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DAyepn__Wei"
   },
   "source": [
    "### Download and prepare the Tiny-Imagenet dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sP9A0oukeu-p"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpTE3kK99fRl"
   },
   "outputs": [],
   "source": [
    "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip && unzip -qq tiny-imagenet-200.zip && rm tiny-imagenet-200.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "EofbttFn4blD"
   },
   "outputs": [],
   "source": [
    "## DONOT modify the code in this cell!\n",
    "## For the curiosu: We're re-organising the files into standard format for easier dataloading\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import copy\n",
    "\n",
    "categories = os.listdir('tiny-imagenet-200/train/')\n",
    "assert len(categories) == 200\n",
    "#for each in categories:\n",
    "    #print(each)\n",
    "    #os.mkdir(f'tiny-imagenet-200/val/{each}')\n",
    "\n",
    "df = pd.read_csv('tiny-imagenet-200/val/val_annotations.txt', delimiter='\\t', header=None)\n",
    "\n",
    "label_to_cat = dict(zip(df[0], df[1]))\n",
    "\n",
    "for each in glob.glob('tiny-imagenet-200/val/images/*.JPEG'):\n",
    "    src = copy.copy(each)\n",
    "    fl_name = each.split('/')[-1]\n",
    "    #print(fl_name)\n",
    "    dest = each.replace('images', label_to_cat[fl_name])\n",
    "    shutil.move(src, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YxCJ4hnD-wtR"
   },
   "outputs": [],
   "source": [
    "!rm -rf tiny-imagenet-200/val/images/ tiny-imagenet-200/val/val_annotations.txt tiny-imagenet-200/test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "cXiv0RdK9b1V"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wc' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# This is shold return 10000\n",
    "!cd tiny-imagenet-200/val/ && find . -type f | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U99Y-WUU_jub"
   },
   "source": [
    "### Model building\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "OMbbVtaKcYLb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import timeit\n",
    "import unittest\n",
    "\n",
    "## Please DONOT remove these lines. \n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "RDnFqg7D-OKX"
   },
   "outputs": [],
   "source": [
    "# check availability of GPU and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# define a set of transforms for preparing the dataset\n",
    "# use mean and std of imagenet dataset\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=8),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(), normalize\n",
    "       ]) \n",
    "          # use random-resized-crop with a image size of 224\n",
    "          # flip the image horizontally (use pytorch random horizontal flip)\n",
    "          # convert the image to a pytorch tensor\n",
    "          # normalise the image\n",
    "            \n",
    "# define transforms for the test data: Should they be same as the one used for train? \n",
    "transform_test = transforms.Compose([\n",
    "          transforms.Resize((256,256)),\n",
    "          transforms.CenterCrop((224,224)),\n",
    "          transforms.ToTensor(), normalize])\n",
    "          # re-size the images to 256x256\n",
    "          # center-crop the 256 images to 224x224\n",
    "          # convert the image to a pytorch tensor\n",
    "          # normalise the image\n",
    "  \n",
    "\n",
    "use_cuda = torch.cuda.is_available() # if you have acess to a GPU, enabble it to speed the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "z1eesbGmcfcH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows-SSD\n",
      " Volume Serial Number is E827-B835\n",
      "\n",
      " Directory of C:\\Users\\manas\\OneDrive - Clemson University\\Documents\\ICAR\\DL4CV\\Deep-learning-for-vision-NPTEL\\Assignment 3\\Programming Assignment 3\n",
      "\n",
      "\n",
      " Directory of C:\\Users\\manas\\OneDrive - Clemson University\\Documents\\ICAR\\DL4CV\\Deep-learning-for-vision-NPTEL\\Assignment 3\\Programming Assignment 3\n",
      "\n",
      "\n",
      " Directory of C:\\Users\\manas\\OneDrive - Clemson University\\Documents\\ICAR\\DL4CV\\Deep-learning-for-vision-NPTEL\\Assignment 3\\Programming Assignment 3\n",
      "\n",
      "\n",
      " Directory of C:\\Users\\manas\\OneDrive - Clemson University\\Documents\\ICAR\\DL4CV\\Deep-learning-for-vision-NPTEL\\Assignment 3\\Programming Assignment 3\n",
      "\n",
      "\n",
      " Directory of C:\\Users\\manas\\OneDrive - Clemson University\\Documents\\ICAR\\DL4CV\\Deep-learning-for-vision-NPTEL\\Assignment 3\\Programming Assignment 3\\tiny-imagenet-200\n",
      "\n",
      "11/02/2020  11:19 PM    <DIR>          .\n",
      "11/02/2020  11:19 PM    <DIR>          ..\n",
      "11/02/2020  10:47 PM    <DIR>          test\n",
      "11/02/2020  11:19 PM    <DIR>          train\n",
      "11/03/2020  02:11 PM    <DIR>          val\n",
      "11/02/2020  10:47 PM             2,000 wnids.txt\n",
      "11/02/2020  10:47 PM         2,655,750 words.txt\n",
      "               2 File(s)      2,657,750 bytes\n",
      "\n",
      " Directory of C:\\Users\\manas\\OneDrive - Clemson University\\Documents\\ICAR\\DL4CV\\Deep-learning-for-vision-NPTEL\\Assignment 3\\Programming Assignment 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File Not Found\n",
      "File Not Found\n"
     ]
    }
   ],
   "source": [
    "!dir # You should see tiny-imagenet-200 folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "kikXzp0WBY_M"
   },
   "outputs": [],
   "source": [
    "# Load the training, test datasets using `torchvision.datasets.ImageFolder`\n",
    "#### YOUR CODE STARTS HERE ####\n",
    "train_dataset = torchvision.datasets.ImageFolder(root = \"./tiny-imagenet-200\", transform=transform_train)\n",
    "test_dataset = torchvision.datasets.ImageFolder(root = \"./tiny-imagenet-200\", transform=transform_test)\n",
    "#### YOUR CODE ENDS HERE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "dCDZTsdqBYTe"
   },
   "outputs": [],
   "source": [
    "# create dataloaders for training and test datasets\n",
    "# use a batch size of 32 and set shuffle=True for the training set\n",
    "#### YOUR CODE STARTS HERE ####\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "#### YOUR CODE ENDS HERE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "UOp1w3DUcm4t"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "      #### YOUR CODE STARTS HERE ####\n",
    "        # send the image, target to the device\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        # flush out the gradients stored in optimizer\n",
    "        model.zero_grad()\n",
    "        # pass the image to the model and assign the output to variable named output\n",
    "        output = model(data)\n",
    "        # calculate the loss (use cross entropy in pytorch)\n",
    "        loss = nn.CrossEntropyLoss()(output, target)\n",
    "        # do a backward pass\n",
    "        loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "      #### YOUR CODE ENDS HERE ####\n",
    "        if batch_idx % 20 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "zslYzJtrcshc"
   },
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "          ### YOUR CODE STARTS HERE ####\n",
    "            # send the image, target to the device\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            # pass the image to the model and assign the output to variable named output\n",
    "            output = model(data)\n",
    "            test_loss += nn.CrossEntropyLoss()(output, target).item()\n",
    "          #### YOUR CODE ENDS HERE ####\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ditBA8j2lyxA"
   },
   "source": [
    "### Question-2\n",
    "\n",
    "What are the number of input features for the final FC layer in Resnet-50? (Hint: Use the code below)\n",
    "\n",
    "1. 1024\n",
    "2. 512\n",
    "3. 784\n",
    "4. 2048\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "T3djBkDXctnx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/120000 (0%)]\tLoss: 5.775721\n",
      "Train Epoch: 1 [640/120000 (1%)]\tLoss: 0.660510\n",
      "Train Epoch: 1 [1280/120000 (1%)]\tLoss: 2.725136\n",
      "Train Epoch: 1 [1920/120000 (2%)]\tLoss: 0.412715\n",
      "Train Epoch: 1 [2560/120000 (2%)]\tLoss: 1.124153\n",
      "Train Epoch: 1 [3200/120000 (3%)]\tLoss: 0.289021\n",
      "Train Epoch: 1 [3840/120000 (3%)]\tLoss: 0.759166\n",
      "Train Epoch: 1 [4480/120000 (4%)]\tLoss: 0.278141\n",
      "Train Epoch: 1 [5120/120000 (4%)]\tLoss: 0.735384\n",
      "Train Epoch: 1 [5760/120000 (5%)]\tLoss: 0.462758\n",
      "Train Epoch: 1 [6400/120000 (5%)]\tLoss: 0.480596\n",
      "Train Epoch: 1 [7040/120000 (6%)]\tLoss: 0.760114\n",
      "Train Epoch: 1 [7680/120000 (6%)]\tLoss: 0.375575\n",
      "Train Epoch: 1 [8320/120000 (7%)]\tLoss: 0.476999\n",
      "Train Epoch: 1 [8960/120000 (7%)]\tLoss: 0.721253\n",
      "Train Epoch: 1 [9600/120000 (8%)]\tLoss: 1.013696\n",
      "Train Epoch: 1 [10240/120000 (9%)]\tLoss: 0.607429\n",
      "Train Epoch: 1 [10880/120000 (9%)]\tLoss: 0.505223\n",
      "Train Epoch: 1 [11520/120000 (10%)]\tLoss: 1.797929\n",
      "Train Epoch: 1 [12160/120000 (10%)]\tLoss: 0.493883\n",
      "Train Epoch: 1 [12800/120000 (11%)]\tLoss: 0.587784\n",
      "Train Epoch: 1 [13440/120000 (11%)]\tLoss: 0.612242\n",
      "Train Epoch: 1 [14080/120000 (12%)]\tLoss: 0.389231\n",
      "Train Epoch: 1 [14720/120000 (12%)]\tLoss: 0.878107\n",
      "Train Epoch: 1 [15360/120000 (13%)]\tLoss: 0.743554\n",
      "Train Epoch: 1 [16000/120000 (13%)]\tLoss: 0.642385\n",
      "Train Epoch: 1 [16640/120000 (14%)]\tLoss: 0.609797\n",
      "Train Epoch: 1 [17280/120000 (14%)]\tLoss: 0.659267\n",
      "Train Epoch: 1 [17920/120000 (15%)]\tLoss: 0.814429\n",
      "Train Epoch: 1 [18560/120000 (15%)]\tLoss: 0.517146\n",
      "Train Epoch: 1 [19200/120000 (16%)]\tLoss: 0.507407\n",
      "Train Epoch: 1 [19840/120000 (17%)]\tLoss: 1.042504\n",
      "Train Epoch: 1 [20480/120000 (17%)]\tLoss: 0.696884\n",
      "Train Epoch: 1 [21120/120000 (18%)]\tLoss: 0.497539\n",
      "Train Epoch: 1 [21760/120000 (18%)]\tLoss: 0.484169\n",
      "Train Epoch: 1 [22400/120000 (19%)]\tLoss: 0.994891\n",
      "Train Epoch: 1 [23040/120000 (19%)]\tLoss: 0.695478\n",
      "Train Epoch: 1 [23680/120000 (20%)]\tLoss: 0.466645\n",
      "Train Epoch: 1 [24320/120000 (20%)]\tLoss: 0.615269\n",
      "Train Epoch: 1 [24960/120000 (21%)]\tLoss: 0.659979\n",
      "Train Epoch: 1 [25600/120000 (21%)]\tLoss: 0.789734\n",
      "Train Epoch: 1 [26240/120000 (22%)]\tLoss: 0.752284\n",
      "Train Epoch: 1 [26880/120000 (22%)]\tLoss: 1.211004\n",
      "Train Epoch: 1 [27520/120000 (23%)]\tLoss: 0.459130\n",
      "Train Epoch: 1 [28160/120000 (23%)]\tLoss: 1.137086\n",
      "Train Epoch: 1 [28800/120000 (24%)]\tLoss: 0.725955\n",
      "Train Epoch: 1 [29440/120000 (25%)]\tLoss: 0.359319\n",
      "Train Epoch: 1 [30080/120000 (25%)]\tLoss: 0.674631\n",
      "Train Epoch: 1 [30720/120000 (26%)]\tLoss: 0.545611\n",
      "Train Epoch: 1 [31360/120000 (26%)]\tLoss: 0.716798\n",
      "Train Epoch: 1 [32000/120000 (27%)]\tLoss: 0.543885\n",
      "Train Epoch: 1 [32640/120000 (27%)]\tLoss: 0.601787\n",
      "Train Epoch: 1 [33280/120000 (28%)]\tLoss: 0.687255\n",
      "Train Epoch: 1 [33920/120000 (28%)]\tLoss: 0.480855\n",
      "Train Epoch: 1 [34560/120000 (29%)]\tLoss: 0.617276\n",
      "Train Epoch: 1 [35200/120000 (29%)]\tLoss: 0.558788\n",
      "Train Epoch: 1 [35840/120000 (30%)]\tLoss: 0.777906\n",
      "Train Epoch: 1 [36480/120000 (30%)]\tLoss: 0.513403\n",
      "Train Epoch: 1 [37120/120000 (31%)]\tLoss: 0.557526\n",
      "Train Epoch: 1 [37760/120000 (31%)]\tLoss: 0.792490\n",
      "Train Epoch: 1 [38400/120000 (32%)]\tLoss: 0.719145\n",
      "Train Epoch: 1 [39040/120000 (33%)]\tLoss: 0.773000\n",
      "Train Epoch: 1 [39680/120000 (33%)]\tLoss: 0.669530\n",
      "Train Epoch: 1 [40320/120000 (34%)]\tLoss: 0.478700\n",
      "Train Epoch: 1 [40960/120000 (34%)]\tLoss: 0.246732\n",
      "Train Epoch: 1 [41600/120000 (35%)]\tLoss: 0.414923\n",
      "Train Epoch: 1 [42240/120000 (35%)]\tLoss: 0.772582\n",
      "Train Epoch: 1 [42880/120000 (36%)]\tLoss: 0.542661\n",
      "Train Epoch: 1 [43520/120000 (36%)]\tLoss: 0.605530\n",
      "Train Epoch: 1 [44160/120000 (37%)]\tLoss: 0.529217\n",
      "Train Epoch: 1 [44800/120000 (37%)]\tLoss: 0.505151\n",
      "Train Epoch: 1 [45440/120000 (38%)]\tLoss: 0.350956\n",
      "Train Epoch: 1 [46080/120000 (38%)]\tLoss: 1.016004\n",
      "Train Epoch: 1 [46720/120000 (39%)]\tLoss: 0.291402\n",
      "Train Epoch: 1 [47360/120000 (39%)]\tLoss: 0.615731\n",
      "Train Epoch: 1 [48000/120000 (40%)]\tLoss: 0.391593\n",
      "Train Epoch: 1 [48640/120000 (41%)]\tLoss: 0.278037\n",
      "Train Epoch: 1 [49280/120000 (41%)]\tLoss: 0.624169\n",
      "Train Epoch: 1 [49920/120000 (42%)]\tLoss: 0.537404\n",
      "Train Epoch: 1 [50560/120000 (42%)]\tLoss: 0.625666\n",
      "Train Epoch: 1 [51200/120000 (43%)]\tLoss: 0.390046\n",
      "Train Epoch: 1 [51840/120000 (43%)]\tLoss: 0.553179\n",
      "Train Epoch: 1 [52480/120000 (44%)]\tLoss: 0.756027\n",
      "Train Epoch: 1 [53120/120000 (44%)]\tLoss: 0.543439\n",
      "Train Epoch: 1 [53760/120000 (45%)]\tLoss: 0.545630\n",
      "Train Epoch: 1 [54400/120000 (45%)]\tLoss: 0.415662\n",
      "Train Epoch: 1 [55040/120000 (46%)]\tLoss: 0.558664\n",
      "Train Epoch: 1 [55680/120000 (46%)]\tLoss: 0.558182\n",
      "Train Epoch: 1 [56320/120000 (47%)]\tLoss: 0.732206\n",
      "Train Epoch: 1 [56960/120000 (47%)]\tLoss: 0.705119\n",
      "Train Epoch: 1 [57600/120000 (48%)]\tLoss: 0.700455\n",
      "Train Epoch: 1 [58240/120000 (49%)]\tLoss: 0.556547\n",
      "Train Epoch: 1 [58880/120000 (49%)]\tLoss: 0.704397\n",
      "Train Epoch: 1 [59520/120000 (50%)]\tLoss: 0.538846\n",
      "Train Epoch: 1 [60160/120000 (50%)]\tLoss: 0.551887\n",
      "Train Epoch: 1 [60800/120000 (51%)]\tLoss: 0.769284\n",
      "Train Epoch: 1 [61440/120000 (51%)]\tLoss: 0.708524\n",
      "Train Epoch: 1 [62080/120000 (52%)]\tLoss: 0.409408\n",
      "Train Epoch: 1 [62720/120000 (52%)]\tLoss: 0.845778\n",
      "Train Epoch: 1 [63360/120000 (53%)]\tLoss: 0.474846\n",
      "Train Epoch: 1 [64000/120000 (53%)]\tLoss: 0.627703\n",
      "Train Epoch: 1 [64640/120000 (54%)]\tLoss: 0.414716\n",
      "Train Epoch: 1 [65280/120000 (54%)]\tLoss: 0.554842\n",
      "Train Epoch: 1 [65920/120000 (55%)]\tLoss: 0.486732\n",
      "Train Epoch: 1 [66560/120000 (55%)]\tLoss: 0.554501\n",
      "Train Epoch: 1 [67200/120000 (56%)]\tLoss: 0.635956\n",
      "Train Epoch: 1 [67840/120000 (57%)]\tLoss: 0.398605\n",
      "Train Epoch: 1 [68480/120000 (57%)]\tLoss: 0.619592\n",
      "Train Epoch: 1 [69120/120000 (58%)]\tLoss: 0.474281\n",
      "Train Epoch: 1 [69760/120000 (58%)]\tLoss: 0.555857\n",
      "Train Epoch: 1 [70400/120000 (59%)]\tLoss: 0.612680\n",
      "Train Epoch: 1 [71040/120000 (59%)]\tLoss: 0.469886\n",
      "Train Epoch: 1 [71680/120000 (60%)]\tLoss: 0.261758\n",
      "Train Epoch: 1 [72320/120000 (60%)]\tLoss: 0.419452\n",
      "Train Epoch: 1 [72960/120000 (61%)]\tLoss: 0.653599\n",
      "Train Epoch: 1 [73600/120000 (61%)]\tLoss: 0.745199\n",
      "Train Epoch: 1 [74240/120000 (62%)]\tLoss: 0.873183\n",
      "Train Epoch: 1 [74880/120000 (62%)]\tLoss: 0.491622\n",
      "Train Epoch: 1 [75520/120000 (63%)]\tLoss: 0.632994\n",
      "Train Epoch: 1 [76160/120000 (63%)]\tLoss: 0.406856\n",
      "Train Epoch: 1 [76800/120000 (64%)]\tLoss: 0.619901\n",
      "Train Epoch: 1 [77440/120000 (65%)]\tLoss: 0.917343\n",
      "Train Epoch: 1 [78080/120000 (65%)]\tLoss: 0.621211\n",
      "Train Epoch: 1 [78720/120000 (66%)]\tLoss: 0.563535\n",
      "Train Epoch: 1 [79360/120000 (66%)]\tLoss: 0.387472\n",
      "Train Epoch: 1 [80000/120000 (67%)]\tLoss: 0.582542\n",
      "Train Epoch: 1 [80640/120000 (67%)]\tLoss: 0.473489\n",
      "Train Epoch: 1 [81280/120000 (68%)]\tLoss: 0.736593\n",
      "Train Epoch: 1 [81920/120000 (68%)]\tLoss: 0.315102\n",
      "Train Epoch: 1 [82560/120000 (69%)]\tLoss: 0.591086\n",
      "Train Epoch: 1 [83200/120000 (69%)]\tLoss: 0.902626\n",
      "Train Epoch: 1 [83840/120000 (70%)]\tLoss: 0.468986\n",
      "Train Epoch: 1 [84480/120000 (70%)]\tLoss: 0.641213\n",
      "Train Epoch: 1 [85120/120000 (71%)]\tLoss: 0.521488\n",
      "Train Epoch: 1 [85760/120000 (71%)]\tLoss: 0.604930\n",
      "Train Epoch: 1 [86400/120000 (72%)]\tLoss: 0.458436\n",
      "Train Epoch: 1 [87040/120000 (73%)]\tLoss: 0.354382\n",
      "Train Epoch: 1 [87680/120000 (73%)]\tLoss: 0.618028\n",
      "Train Epoch: 1 [88320/120000 (74%)]\tLoss: 0.683934\n",
      "Train Epoch: 1 [88960/120000 (74%)]\tLoss: 0.376858\n",
      "Train Epoch: 1 [89600/120000 (75%)]\tLoss: 0.615961\n",
      "Train Epoch: 1 [90240/120000 (75%)]\tLoss: 0.475381\n",
      "Train Epoch: 1 [90880/120000 (76%)]\tLoss: 0.415554\n",
      "Train Epoch: 1 [91520/120000 (76%)]\tLoss: 0.839933\n",
      "Train Epoch: 1 [92160/120000 (77%)]\tLoss: 0.673008\n",
      "Train Epoch: 1 [92800/120000 (77%)]\tLoss: 0.665859\n",
      "Train Epoch: 1 [93440/120000 (78%)]\tLoss: 0.566114\n",
      "Train Epoch: 1 [94080/120000 (78%)]\tLoss: 0.478364\n",
      "Train Epoch: 1 [94720/120000 (79%)]\tLoss: 0.796087\n",
      "Train Epoch: 1 [95360/120000 (79%)]\tLoss: 0.541305\n",
      "Train Epoch: 1 [96000/120000 (80%)]\tLoss: 0.463296\n",
      "Train Epoch: 1 [96640/120000 (81%)]\tLoss: 0.670693\n",
      "Train Epoch: 1 [97280/120000 (81%)]\tLoss: 0.541979\n",
      "Train Epoch: 1 [97920/120000 (82%)]\tLoss: 0.551579\n",
      "Train Epoch: 1 [98560/120000 (82%)]\tLoss: 0.556973\n",
      "Train Epoch: 1 [99200/120000 (83%)]\tLoss: 0.758685\n",
      "Train Epoch: 1 [99840/120000 (83%)]\tLoss: 0.622641\n",
      "Train Epoch: 1 [100480/120000 (84%)]\tLoss: 0.686824\n",
      "Train Epoch: 1 [101120/120000 (84%)]\tLoss: 0.698037\n",
      "Train Epoch: 1 [101760/120000 (85%)]\tLoss: 0.476852\n",
      "Train Epoch: 1 [102400/120000 (85%)]\tLoss: 0.679340\n",
      "Train Epoch: 1 [103040/120000 (86%)]\tLoss: 0.475570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [103680/120000 (86%)]\tLoss: 0.746921\n",
      "Train Epoch: 1 [104320/120000 (87%)]\tLoss: 0.768431\n",
      "Train Epoch: 1 [104960/120000 (87%)]\tLoss: 0.668789\n",
      "Train Epoch: 1 [105600/120000 (88%)]\tLoss: 0.341037\n",
      "Train Epoch: 1 [106240/120000 (89%)]\tLoss: 1.200221\n",
      "Train Epoch: 1 [106880/120000 (89%)]\tLoss: 0.895387\n",
      "Train Epoch: 1 [107520/120000 (90%)]\tLoss: 0.565530\n",
      "Train Epoch: 1 [108160/120000 (90%)]\tLoss: 0.605580\n",
      "Train Epoch: 1 [108800/120000 (91%)]\tLoss: 0.622277\n",
      "Train Epoch: 1 [109440/120000 (91%)]\tLoss: 0.835397\n",
      "Train Epoch: 1 [110080/120000 (92%)]\tLoss: 0.536417\n",
      "Train Epoch: 1 [110720/120000 (92%)]\tLoss: 0.531205\n",
      "Train Epoch: 1 [111360/120000 (93%)]\tLoss: 0.324356\n",
      "Train Epoch: 1 [112000/120000 (93%)]\tLoss: 0.697812\n",
      "Train Epoch: 1 [112640/120000 (94%)]\tLoss: 0.672885\n",
      "Train Epoch: 1 [113280/120000 (94%)]\tLoss: 0.598334\n",
      "Train Epoch: 1 [113920/120000 (95%)]\tLoss: 0.547910\n",
      "Train Epoch: 1 [114560/120000 (95%)]\tLoss: 0.844955\n",
      "Train Epoch: 1 [115200/120000 (96%)]\tLoss: 0.548256\n",
      "Train Epoch: 1 [115840/120000 (97%)]\tLoss: 0.631966\n",
      "Train Epoch: 1 [116480/120000 (97%)]\tLoss: 0.413872\n",
      "Train Epoch: 1 [117120/120000 (98%)]\tLoss: 0.492743\n",
      "Train Epoch: 1 [117760/120000 (98%)]\tLoss: 0.709228\n",
      "Train Epoch: 1 [118400/120000 (99%)]\tLoss: 0.554838\n",
      "Train Epoch: 1 [119040/120000 (99%)]\tLoss: 0.689855\n",
      "Train Epoch: 1 [119680/120000 (100%)]\tLoss: 0.677197\n",
      "\n",
      "Test set: Average loss: 0.0416, Accuracy: 100000/120000 (83%)\n",
      "\n",
      "Train Epoch: 2 [0/120000 (0%)]\tLoss: 0.623734\n",
      "Train Epoch: 2 [640/120000 (1%)]\tLoss: 0.603346\n",
      "Train Epoch: 2 [1280/120000 (1%)]\tLoss: 0.629690\n",
      "Train Epoch: 2 [1920/120000 (2%)]\tLoss: 0.550830\n",
      "Train Epoch: 2 [2560/120000 (2%)]\tLoss: 0.782523\n",
      "Train Epoch: 2 [3200/120000 (3%)]\tLoss: 0.713616\n",
      "Train Epoch: 2 [3840/120000 (3%)]\tLoss: 0.818087\n",
      "Train Epoch: 2 [4480/120000 (4%)]\tLoss: 0.609805\n",
      "Train Epoch: 2 [5120/120000 (4%)]\tLoss: 0.337208\n",
      "Train Epoch: 2 [5760/120000 (5%)]\tLoss: 0.575074\n",
      "Train Epoch: 2 [6400/120000 (5%)]\tLoss: 0.765342\n",
      "Train Epoch: 2 [7040/120000 (6%)]\tLoss: 0.699007\n",
      "Train Epoch: 2 [7680/120000 (6%)]\tLoss: 0.477255\n",
      "Train Epoch: 2 [8320/120000 (7%)]\tLoss: 0.699312\n",
      "Train Epoch: 2 [8960/120000 (7%)]\tLoss: 0.424408\n",
      "Train Epoch: 2 [9600/120000 (8%)]\tLoss: 0.429334\n",
      "Train Epoch: 2 [10240/120000 (9%)]\tLoss: 0.511835\n",
      "Train Epoch: 2 [10880/120000 (9%)]\tLoss: 0.852033\n",
      "Train Epoch: 2 [11520/120000 (10%)]\tLoss: 0.904055\n",
      "Train Epoch: 2 [12160/120000 (10%)]\tLoss: 0.774402\n",
      "Train Epoch: 2 [12800/120000 (11%)]\tLoss: 0.568687\n",
      "Train Epoch: 2 [13440/120000 (11%)]\tLoss: 0.604380\n",
      "Train Epoch: 2 [14080/120000 (12%)]\tLoss: 0.461827\n",
      "Train Epoch: 2 [14720/120000 (12%)]\tLoss: 0.741316\n",
      "Train Epoch: 2 [15360/120000 (13%)]\tLoss: 0.955358\n",
      "Train Epoch: 2 [16000/120000 (13%)]\tLoss: 0.786066\n",
      "Train Epoch: 2 [16640/120000 (14%)]\tLoss: 0.843277\n",
      "Train Epoch: 2 [17280/120000 (14%)]\tLoss: 0.275392\n",
      "Train Epoch: 2 [17920/120000 (15%)]\tLoss: 0.768938\n",
      "Train Epoch: 2 [18560/120000 (15%)]\tLoss: 0.540074\n",
      "Train Epoch: 2 [19200/120000 (16%)]\tLoss: 0.664085\n",
      "Train Epoch: 2 [19840/120000 (17%)]\tLoss: 0.610563\n",
      "Train Epoch: 2 [20480/120000 (17%)]\tLoss: 0.619835\n",
      "Train Epoch: 2 [21120/120000 (18%)]\tLoss: 0.692005\n",
      "Train Epoch: 2 [21760/120000 (18%)]\tLoss: 0.491512\n",
      "Train Epoch: 2 [22400/120000 (19%)]\tLoss: 0.473848\n",
      "Train Epoch: 2 [23040/120000 (19%)]\tLoss: 0.567153\n",
      "Train Epoch: 2 [23680/120000 (20%)]\tLoss: 0.570054\n",
      "Train Epoch: 2 [24320/120000 (20%)]\tLoss: 0.480697\n",
      "Train Epoch: 2 [24960/120000 (21%)]\tLoss: 0.686372\n",
      "Train Epoch: 2 [25600/120000 (21%)]\tLoss: 0.470073\n",
      "Train Epoch: 2 [26240/120000 (22%)]\tLoss: 0.697764\n",
      "Train Epoch: 2 [26880/120000 (22%)]\tLoss: 0.857955\n",
      "Train Epoch: 2 [27520/120000 (23%)]\tLoss: 0.543970\n",
      "Train Epoch: 2 [28160/120000 (23%)]\tLoss: 0.476825\n",
      "Train Epoch: 2 [28800/120000 (24%)]\tLoss: 0.396674\n",
      "Train Epoch: 2 [29440/120000 (25%)]\tLoss: 0.620602\n",
      "Train Epoch: 2 [30080/120000 (25%)]\tLoss: 0.477466\n",
      "Train Epoch: 2 [30720/120000 (26%)]\tLoss: 0.396861\n",
      "Train Epoch: 2 [31360/120000 (26%)]\tLoss: 0.545164\n",
      "Train Epoch: 2 [32000/120000 (27%)]\tLoss: 0.722181\n",
      "Train Epoch: 2 [32640/120000 (27%)]\tLoss: 0.541841\n",
      "Train Epoch: 2 [33280/120000 (28%)]\tLoss: 0.466736\n",
      "Train Epoch: 2 [33920/120000 (28%)]\tLoss: 0.769568\n",
      "Train Epoch: 2 [34560/120000 (29%)]\tLoss: 0.541578\n",
      "Train Epoch: 2 [35200/120000 (29%)]\tLoss: 0.336865\n",
      "Train Epoch: 2 [35840/120000 (30%)]\tLoss: 0.275026\n",
      "Train Epoch: 2 [36480/120000 (30%)]\tLoss: 0.695027\n",
      "Train Epoch: 2 [37120/120000 (31%)]\tLoss: 0.540386\n",
      "Train Epoch: 2 [37760/120000 (31%)]\tLoss: 0.486109\n",
      "Train Epoch: 2 [38400/120000 (32%)]\tLoss: 0.751954\n",
      "Train Epoch: 2 [39040/120000 (33%)]\tLoss: 0.610022\n",
      "Train Epoch: 2 [39680/120000 (33%)]\tLoss: 0.311821\n",
      "Train Epoch: 2 [40320/120000 (34%)]\tLoss: 0.575987\n",
      "Train Epoch: 2 [40960/120000 (34%)]\tLoss: 0.411590\n",
      "Train Epoch: 2 [41600/120000 (35%)]\tLoss: 0.468045\n",
      "Train Epoch: 2 [42240/120000 (35%)]\tLoss: 0.541166\n",
      "Train Epoch: 2 [42880/120000 (36%)]\tLoss: 0.836804\n",
      "Train Epoch: 2 [43520/120000 (36%)]\tLoss: 0.475151\n",
      "Train Epoch: 2 [44160/120000 (37%)]\tLoss: 0.879062\n",
      "Train Epoch: 2 [44800/120000 (37%)]\tLoss: 0.551604\n",
      "Train Epoch: 2 [45440/120000 (38%)]\tLoss: 0.823121\n",
      "Train Epoch: 2 [46080/120000 (38%)]\tLoss: 0.542259\n",
      "Train Epoch: 2 [46720/120000 (39%)]\tLoss: 0.482832\n",
      "Train Epoch: 2 [47360/120000 (39%)]\tLoss: 0.620081\n",
      "Train Epoch: 2 [48000/120000 (40%)]\tLoss: 0.469874\n",
      "Train Epoch: 2 [48640/120000 (41%)]\tLoss: 0.547357\n",
      "Train Epoch: 2 [49280/120000 (41%)]\tLoss: 0.474009\n",
      "Train Epoch: 2 [49920/120000 (42%)]\tLoss: 0.535990\n",
      "Train Epoch: 2 [50560/120000 (42%)]\tLoss: 0.328285\n",
      "Train Epoch: 2 [51200/120000 (43%)]\tLoss: 0.821709\n",
      "Train Epoch: 2 [51840/120000 (43%)]\tLoss: 0.547449\n",
      "Train Epoch: 2 [52480/120000 (44%)]\tLoss: 0.616719\n",
      "Train Epoch: 2 [53120/120000 (44%)]\tLoss: 0.467024\n",
      "Train Epoch: 2 [53760/120000 (45%)]\tLoss: 0.603569\n",
      "Train Epoch: 2 [54400/120000 (45%)]\tLoss: 0.620544\n",
      "Train Epoch: 2 [55040/120000 (46%)]\tLoss: 0.555748\n",
      "Train Epoch: 2 [55680/120000 (46%)]\tLoss: 0.325092\n",
      "Train Epoch: 2 [56320/120000 (47%)]\tLoss: 0.698543\n",
      "Train Epoch: 2 [56960/120000 (47%)]\tLoss: 0.620655\n",
      "Train Epoch: 2 [57600/120000 (48%)]\tLoss: 0.470872\n",
      "Train Epoch: 2 [58240/120000 (49%)]\tLoss: 0.692208\n",
      "Train Epoch: 2 [58880/120000 (49%)]\tLoss: 0.307294\n",
      "Train Epoch: 2 [59520/120000 (50%)]\tLoss: 0.468100\n",
      "Train Epoch: 2 [60160/120000 (50%)]\tLoss: 0.469966\n",
      "Train Epoch: 2 [60800/120000 (51%)]\tLoss: 0.470376\n",
      "Train Epoch: 2 [61440/120000 (51%)]\tLoss: 0.478874\n",
      "Train Epoch: 2 [62080/120000 (52%)]\tLoss: 0.547153\n",
      "Train Epoch: 2 [62720/120000 (52%)]\tLoss: 0.378896\n",
      "Train Epoch: 2 [63360/120000 (53%)]\tLoss: 0.920971\n",
      "Train Epoch: 2 [64000/120000 (53%)]\tLoss: 0.689105\n",
      "Train Epoch: 2 [64640/120000 (54%)]\tLoss: 0.535626\n",
      "Train Epoch: 2 [65280/120000 (54%)]\tLoss: 0.549257\n",
      "Train Epoch: 2 [65920/120000 (55%)]\tLoss: 0.545273\n",
      "Train Epoch: 2 [66560/120000 (55%)]\tLoss: 0.386948\n",
      "Train Epoch: 2 [67200/120000 (56%)]\tLoss: 0.535680\n",
      "Train Epoch: 2 [67840/120000 (57%)]\tLoss: 0.698653\n",
      "Train Epoch: 2 [68480/120000 (57%)]\tLoss: 0.472699\n",
      "Train Epoch: 2 [69120/120000 (58%)]\tLoss: 0.768881\n",
      "Train Epoch: 2 [69760/120000 (58%)]\tLoss: 0.249067\n",
      "Train Epoch: 2 [70400/120000 (59%)]\tLoss: 0.547500\n",
      "Train Epoch: 2 [71040/120000 (59%)]\tLoss: 0.611110\n",
      "Train Epoch: 2 [71680/120000 (60%)]\tLoss: 0.612488\n",
      "Train Epoch: 2 [72320/120000 (60%)]\tLoss: 0.456187\n",
      "Train Epoch: 2 [72960/120000 (61%)]\tLoss: 0.477069\n",
      "Train Epoch: 2 [73600/120000 (61%)]\tLoss: 0.614877\n",
      "Train Epoch: 2 [74240/120000 (62%)]\tLoss: 0.306758\n",
      "Train Epoch: 2 [74880/120000 (62%)]\tLoss: 0.535430\n",
      "Train Epoch: 2 [75520/120000 (63%)]\tLoss: 0.612059\n",
      "Train Epoch: 2 [76160/120000 (63%)]\tLoss: 0.542936\n",
      "Train Epoch: 2 [76800/120000 (64%)]\tLoss: 0.536368\n",
      "Train Epoch: 2 [77440/120000 (65%)]\tLoss: 0.951966\n",
      "Train Epoch: 2 [78080/120000 (65%)]\tLoss: 0.401418\n",
      "Train Epoch: 2 [78720/120000 (66%)]\tLoss: 0.683689\n",
      "Train Epoch: 2 [79360/120000 (66%)]\tLoss: 0.547987\n",
      "Train Epoch: 2 [80000/120000 (67%)]\tLoss: 0.542197\n",
      "Train Epoch: 2 [80640/120000 (67%)]\tLoss: 0.312359\n",
      "Train Epoch: 2 [81280/120000 (68%)]\tLoss: 0.842388\n",
      "Train Epoch: 2 [81920/120000 (68%)]\tLoss: 0.479687\n",
      "Train Epoch: 2 [82560/120000 (69%)]\tLoss: 0.685704\n",
      "Train Epoch: 2 [83200/120000 (69%)]\tLoss: 0.535146\n",
      "Train Epoch: 2 [83840/120000 (70%)]\tLoss: 0.361409\n",
      "Train Epoch: 2 [84480/120000 (70%)]\tLoss: 0.536910\n",
      "Train Epoch: 2 [85120/120000 (71%)]\tLoss: 0.542448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [85760/120000 (71%)]\tLoss: 0.259627\n",
      "Train Epoch: 2 [86400/120000 (72%)]\tLoss: 0.544814\n",
      "Train Epoch: 2 [87040/120000 (73%)]\tLoss: 0.528513\n",
      "Train Epoch: 2 [87680/120000 (73%)]\tLoss: 0.419225\n",
      "Train Epoch: 2 [88320/120000 (74%)]\tLoss: 0.903511\n",
      "Train Epoch: 2 [88960/120000 (74%)]\tLoss: 0.631080\n",
      "Train Epoch: 2 [89600/120000 (75%)]\tLoss: 0.547395\n",
      "Train Epoch: 2 [90240/120000 (75%)]\tLoss: 0.545975\n",
      "Train Epoch: 2 [90880/120000 (76%)]\tLoss: 0.684229\n",
      "Train Epoch: 2 [91520/120000 (76%)]\tLoss: 0.538742\n",
      "Train Epoch: 2 [92160/120000 (77%)]\tLoss: 0.401610\n",
      "Train Epoch: 2 [92800/120000 (77%)]\tLoss: 0.398490\n",
      "Train Epoch: 2 [93440/120000 (78%)]\tLoss: 0.385850\n",
      "Train Epoch: 2 [94080/120000 (78%)]\tLoss: 0.402030\n",
      "Train Epoch: 2 [94720/120000 (79%)]\tLoss: 0.591832\n",
      "Train Epoch: 2 [95360/120000 (79%)]\tLoss: 0.542548\n",
      "Train Epoch: 2 [96000/120000 (80%)]\tLoss: 0.549256\n",
      "Train Epoch: 2 [96640/120000 (81%)]\tLoss: 0.484625\n",
      "Train Epoch: 2 [97280/120000 (81%)]\tLoss: 0.545654\n",
      "Train Epoch: 2 [97920/120000 (82%)]\tLoss: 0.390916\n",
      "Train Epoch: 2 [98560/120000 (82%)]\tLoss: 0.688065\n",
      "Train Epoch: 2 [99200/120000 (83%)]\tLoss: 0.811022\n",
      "Train Epoch: 2 [99840/120000 (83%)]\tLoss: 0.481161\n",
      "Train Epoch: 2 [100480/120000 (84%)]\tLoss: 0.389459\n",
      "Train Epoch: 2 [101120/120000 (84%)]\tLoss: 0.857260\n",
      "Train Epoch: 2 [101760/120000 (85%)]\tLoss: 0.466767\n",
      "Train Epoch: 2 [102400/120000 (85%)]\tLoss: 0.280542\n",
      "Train Epoch: 2 [103040/120000 (86%)]\tLoss: 0.313299\n",
      "Train Epoch: 2 [103680/120000 (86%)]\tLoss: 0.477865\n",
      "Train Epoch: 2 [104320/120000 (87%)]\tLoss: 0.544009\n",
      "Train Epoch: 2 [104960/120000 (87%)]\tLoss: 0.685052\n",
      "Train Epoch: 2 [105600/120000 (88%)]\tLoss: 0.470865\n",
      "Train Epoch: 2 [106240/120000 (89%)]\tLoss: 0.544448\n",
      "Train Epoch: 2 [106880/120000 (89%)]\tLoss: 0.539277\n",
      "Train Epoch: 2 [107520/120000 (90%)]\tLoss: 0.543319\n",
      "Train Epoch: 2 [108160/120000 (90%)]\tLoss: 0.624261\n",
      "Train Epoch: 2 [108800/120000 (91%)]\tLoss: 0.274685\n",
      "Train Epoch: 2 [109440/120000 (91%)]\tLoss: 0.777766\n",
      "Train Epoch: 2 [110080/120000 (92%)]\tLoss: 0.669699\n",
      "Train Epoch: 2 [110720/120000 (92%)]\tLoss: 0.452408\n",
      "Train Epoch: 2 [111360/120000 (93%)]\tLoss: 0.616944\n",
      "Train Epoch: 2 [112000/120000 (93%)]\tLoss: 0.612418\n",
      "Train Epoch: 2 [112640/120000 (94%)]\tLoss: 0.613531\n",
      "Train Epoch: 2 [113280/120000 (94%)]\tLoss: 0.484039\n",
      "Train Epoch: 2 [113920/120000 (95%)]\tLoss: 0.759056\n",
      "Train Epoch: 2 [114560/120000 (95%)]\tLoss: 0.762193\n",
      "Train Epoch: 2 [115200/120000 (96%)]\tLoss: 0.733540\n",
      "Train Epoch: 2 [115840/120000 (97%)]\tLoss: 0.406692\n",
      "Train Epoch: 2 [116480/120000 (97%)]\tLoss: 0.702844\n",
      "Train Epoch: 2 [117120/120000 (98%)]\tLoss: 0.452163\n",
      "Train Epoch: 2 [117760/120000 (98%)]\tLoss: 0.555102\n",
      "Train Epoch: 2 [118400/120000 (99%)]\tLoss: 0.544078\n",
      "Train Epoch: 2 [119040/120000 (99%)]\tLoss: 0.274691\n",
      "Train Epoch: 2 [119680/120000 (100%)]\tLoss: 0.479047\n",
      "\n",
      "Test set: Average loss: 0.0357, Accuracy: 100000/120000 (83%)\n",
      "\n",
      "Train Epoch: 3 [0/120000 (0%)]\tLoss: 0.332481\n",
      "Train Epoch: 3 [640/120000 (1%)]\tLoss: 0.761102\n",
      "Train Epoch: 3 [1280/120000 (1%)]\tLoss: 0.614515\n",
      "Train Epoch: 3 [1920/120000 (2%)]\tLoss: 0.458520\n",
      "Train Epoch: 3 [2560/120000 (2%)]\tLoss: 0.476808\n",
      "Train Epoch: 3 [3200/120000 (3%)]\tLoss: 0.386332\n",
      "Train Epoch: 3 [3840/120000 (3%)]\tLoss: 0.770868\n",
      "Train Epoch: 3 [4480/120000 (4%)]\tLoss: 0.855471\n",
      "Train Epoch: 3 [5120/120000 (4%)]\tLoss: 0.411531\n",
      "Train Epoch: 3 [5760/120000 (5%)]\tLoss: 0.612016\n",
      "Train Epoch: 3 [6400/120000 (5%)]\tLoss: 0.540966\n",
      "Train Epoch: 3 [7040/120000 (6%)]\tLoss: 0.467836\n",
      "Train Epoch: 3 [7680/120000 (6%)]\tLoss: 0.550749\n",
      "Train Epoch: 3 [8320/120000 (7%)]\tLoss: 0.463068\n",
      "Train Epoch: 3 [8960/120000 (7%)]\tLoss: 0.325471\n",
      "Train Epoch: 3 [9600/120000 (8%)]\tLoss: 0.756725\n",
      "Train Epoch: 3 [10240/120000 (9%)]\tLoss: 0.470551\n",
      "Train Epoch: 3 [10880/120000 (9%)]\tLoss: 0.694066\n",
      "Train Epoch: 3 [11520/120000 (10%)]\tLoss: 0.681423\n",
      "Train Epoch: 3 [12160/120000 (10%)]\tLoss: 0.275914\n",
      "Train Epoch: 3 [12800/120000 (11%)]\tLoss: 0.617312\n",
      "Train Epoch: 3 [13440/120000 (11%)]\tLoss: 0.542354\n",
      "Train Epoch: 3 [14080/120000 (12%)]\tLoss: 0.740379\n",
      "Train Epoch: 3 [14720/120000 (12%)]\tLoss: 0.964326\n",
      "Train Epoch: 3 [15360/120000 (13%)]\tLoss: 0.540579\n",
      "Train Epoch: 3 [16000/120000 (13%)]\tLoss: 0.680869\n",
      "Train Epoch: 3 [16640/120000 (14%)]\tLoss: 0.763089\n",
      "Train Epoch: 3 [17280/120000 (14%)]\tLoss: 0.762408\n",
      "Train Epoch: 3 [17920/120000 (15%)]\tLoss: 0.813771\n",
      "Train Epoch: 3 [18560/120000 (15%)]\tLoss: 0.538947\n",
      "Train Epoch: 3 [19200/120000 (16%)]\tLoss: 0.605328\n",
      "Train Epoch: 3 [19840/120000 (17%)]\tLoss: 0.623282\n",
      "Train Epoch: 3 [20480/120000 (17%)]\tLoss: 0.482357\n",
      "Train Epoch: 3 [21120/120000 (18%)]\tLoss: 0.472160\n",
      "Train Epoch: 3 [21760/120000 (18%)]\tLoss: 0.391785\n",
      "Train Epoch: 3 [22400/120000 (19%)]\tLoss: 0.691566\n",
      "Train Epoch: 3 [23040/120000 (19%)]\tLoss: 0.474460\n",
      "Train Epoch: 3 [23680/120000 (20%)]\tLoss: 0.471490\n",
      "Train Epoch: 3 [24320/120000 (20%)]\tLoss: 0.750863\n",
      "Train Epoch: 3 [24960/120000 (21%)]\tLoss: 0.478352\n",
      "Train Epoch: 3 [25600/120000 (21%)]\tLoss: 0.619969\n",
      "Train Epoch: 3 [26240/120000 (22%)]\tLoss: 0.616617\n",
      "Train Epoch: 3 [26880/120000 (22%)]\tLoss: 0.684357\n",
      "Train Epoch: 3 [27520/120000 (23%)]\tLoss: 0.471427\n",
      "Train Epoch: 3 [28160/120000 (23%)]\tLoss: 0.546650\n",
      "Train Epoch: 3 [28800/120000 (24%)]\tLoss: 0.542938\n",
      "Train Epoch: 3 [29440/120000 (25%)]\tLoss: 0.746015\n",
      "Train Epoch: 3 [30080/120000 (25%)]\tLoss: 0.614023\n",
      "Train Epoch: 3 [30720/120000 (26%)]\tLoss: 0.837781\n",
      "Train Epoch: 3 [31360/120000 (26%)]\tLoss: 0.777065\n",
      "Train Epoch: 3 [32000/120000 (27%)]\tLoss: 0.467262\n",
      "Train Epoch: 3 [32640/120000 (27%)]\tLoss: 0.776968\n",
      "Train Epoch: 3 [33280/120000 (28%)]\tLoss: 0.246828\n",
      "Train Epoch: 3 [33920/120000 (28%)]\tLoss: 0.168755\n",
      "Train Epoch: 3 [34560/120000 (29%)]\tLoss: 0.540430\n",
      "Train Epoch: 3 [35200/120000 (29%)]\tLoss: 0.614398\n",
      "Train Epoch: 3 [35840/120000 (30%)]\tLoss: 0.692253\n",
      "Train Epoch: 3 [36480/120000 (30%)]\tLoss: 0.616713\n",
      "Train Epoch: 3 [37120/120000 (31%)]\tLoss: 0.894115\n",
      "Train Epoch: 3 [37760/120000 (31%)]\tLoss: 0.472506\n",
      "Train Epoch: 3 [38400/120000 (32%)]\tLoss: 0.261682\n",
      "Train Epoch: 3 [39040/120000 (33%)]\tLoss: 0.335056\n",
      "Train Epoch: 3 [39680/120000 (33%)]\tLoss: 0.761456\n",
      "Train Epoch: 3 [40320/120000 (34%)]\tLoss: 0.548776\n",
      "Train Epoch: 3 [40960/120000 (34%)]\tLoss: 0.399007\n",
      "Train Epoch: 3 [41600/120000 (35%)]\tLoss: 0.761283\n",
      "Train Epoch: 3 [42240/120000 (35%)]\tLoss: 0.468884\n",
      "Train Epoch: 3 [42880/120000 (36%)]\tLoss: 0.615762\n",
      "Train Epoch: 3 [43520/120000 (36%)]\tLoss: 0.486309\n",
      "Train Epoch: 3 [44160/120000 (37%)]\tLoss: 0.681680\n",
      "Train Epoch: 3 [44800/120000 (37%)]\tLoss: 0.749064\n",
      "Train Epoch: 3 [45440/120000 (38%)]\tLoss: 0.749142\n",
      "Train Epoch: 3 [46080/120000 (38%)]\tLoss: 0.544703\n",
      "Train Epoch: 3 [46720/120000 (39%)]\tLoss: 0.805344\n",
      "Train Epoch: 3 [47360/120000 (39%)]\tLoss: 0.531307\n",
      "Train Epoch: 3 [48000/120000 (40%)]\tLoss: 0.333503\n",
      "Train Epoch: 3 [48640/120000 (41%)]\tLoss: 0.615741\n",
      "Train Epoch: 3 [49280/120000 (41%)]\tLoss: 0.763218\n",
      "Train Epoch: 3 [49920/120000 (42%)]\tLoss: 0.541366\n",
      "Train Epoch: 3 [50560/120000 (42%)]\tLoss: 0.612276\n",
      "Train Epoch: 3 [51200/120000 (43%)]\tLoss: 0.679439\n",
      "Train Epoch: 3 [51840/120000 (43%)]\tLoss: 0.464331\n",
      "Train Epoch: 3 [52480/120000 (44%)]\tLoss: 0.613172\n",
      "Train Epoch: 3 [53120/120000 (44%)]\tLoss: 0.403637\n",
      "Train Epoch: 3 [53760/120000 (45%)]\tLoss: 0.747995\n",
      "Train Epoch: 3 [54400/120000 (45%)]\tLoss: 0.904852\n",
      "Train Epoch: 3 [55040/120000 (46%)]\tLoss: 0.474720\n",
      "Train Epoch: 3 [55680/120000 (46%)]\tLoss: 0.680537\n",
      "Train Epoch: 3 [56320/120000 (47%)]\tLoss: 0.544844\n",
      "Train Epoch: 3 [56960/120000 (47%)]\tLoss: 0.684537\n",
      "Train Epoch: 3 [57600/120000 (48%)]\tLoss: 0.543616\n",
      "Train Epoch: 3 [58240/120000 (49%)]\tLoss: 1.051747\n",
      "Train Epoch: 3 [58880/120000 (49%)]\tLoss: 0.414173\n",
      "Train Epoch: 3 [59520/120000 (50%)]\tLoss: 0.687465\n",
      "Train Epoch: 3 [60160/120000 (50%)]\tLoss: 0.820631\n",
      "Train Epoch: 3 [60800/120000 (51%)]\tLoss: 0.820506\n",
      "Train Epoch: 3 [61440/120000 (51%)]\tLoss: 0.746840\n",
      "Train Epoch: 3 [62080/120000 (52%)]\tLoss: 0.612465\n",
      "Train Epoch: 3 [62720/120000 (52%)]\tLoss: 0.397465\n",
      "Train Epoch: 3 [63360/120000 (53%)]\tLoss: 0.829056\n",
      "Train Epoch: 3 [64000/120000 (53%)]\tLoss: 0.340484\n",
      "Train Epoch: 3 [64640/120000 (54%)]\tLoss: 0.828753\n",
      "Train Epoch: 3 [65280/120000 (54%)]\tLoss: 0.466178\n",
      "Train Epoch: 3 [65920/120000 (55%)]\tLoss: 0.746054\n",
      "Train Epoch: 3 [66560/120000 (55%)]\tLoss: 0.465002\n",
      "Train Epoch: 3 [67200/120000 (56%)]\tLoss: 0.538226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [67840/120000 (57%)]\tLoss: 0.749334\n",
      "Train Epoch: 3 [68480/120000 (57%)]\tLoss: 0.422768\n",
      "Train Epoch: 3 [69120/120000 (58%)]\tLoss: 0.480922\n",
      "Train Epoch: 3 [69760/120000 (58%)]\tLoss: 0.901123\n",
      "Train Epoch: 3 [70400/120000 (59%)]\tLoss: 0.628253\n",
      "Train Epoch: 3 [71040/120000 (59%)]\tLoss: 0.471588\n",
      "Train Epoch: 3 [71680/120000 (60%)]\tLoss: 0.684749\n",
      "Train Epoch: 3 [72320/120000 (60%)]\tLoss: 0.543302\n",
      "Train Epoch: 3 [72960/120000 (61%)]\tLoss: 0.751429\n",
      "Train Epoch: 3 [73600/120000 (61%)]\tLoss: 0.764299\n",
      "Train Epoch: 3 [74240/120000 (62%)]\tLoss: 0.628071\n",
      "Train Epoch: 3 [74880/120000 (62%)]\tLoss: 0.404115\n",
      "Train Epoch: 3 [75520/120000 (63%)]\tLoss: 0.323361\n",
      "Train Epoch: 3 [76160/120000 (63%)]\tLoss: 0.617645\n",
      "Train Epoch: 3 [76800/120000 (64%)]\tLoss: 0.615653\n",
      "Train Epoch: 3 [77440/120000 (65%)]\tLoss: 0.540284\n",
      "Train Epoch: 3 [78080/120000 (65%)]\tLoss: 0.690664\n",
      "Train Epoch: 3 [78720/120000 (66%)]\tLoss: 0.758432\n",
      "Train Epoch: 3 [79360/120000 (66%)]\tLoss: 0.471610\n",
      "Train Epoch: 3 [80000/120000 (67%)]\tLoss: 0.542432\n",
      "Train Epoch: 3 [80640/120000 (67%)]\tLoss: 0.407164\n",
      "Train Epoch: 3 [81280/120000 (68%)]\tLoss: 0.407471\n",
      "Train Epoch: 3 [81920/120000 (68%)]\tLoss: 0.613705\n",
      "Train Epoch: 3 [82560/120000 (69%)]\tLoss: 0.683448\n",
      "Train Epoch: 3 [83200/120000 (69%)]\tLoss: 0.410482\n",
      "Train Epoch: 3 [83840/120000 (70%)]\tLoss: 0.471118\n",
      "Train Epoch: 3 [84480/120000 (70%)]\tLoss: 0.467863\n",
      "Train Epoch: 3 [85120/120000 (71%)]\tLoss: 0.690449\n",
      "Train Epoch: 3 [85760/120000 (71%)]\tLoss: 0.610554\n",
      "Train Epoch: 3 [86400/120000 (72%)]\tLoss: 0.682021\n",
      "Train Epoch: 3 [87040/120000 (73%)]\tLoss: 0.401856\n",
      "Train Epoch: 3 [87680/120000 (73%)]\tLoss: 0.616709\n",
      "Train Epoch: 3 [88320/120000 (74%)]\tLoss: 0.691989\n",
      "Train Epoch: 3 [88960/120000 (74%)]\tLoss: 0.465014\n",
      "Train Epoch: 3 [89600/120000 (75%)]\tLoss: 0.404196\n",
      "Train Epoch: 3 [90240/120000 (75%)]\tLoss: 0.364415\n",
      "Train Epoch: 3 [90880/120000 (76%)]\tLoss: 0.368555\n",
      "Train Epoch: 3 [91520/120000 (76%)]\tLoss: 0.494269\n",
      "Train Epoch: 3 [92160/120000 (77%)]\tLoss: 0.790011\n",
      "Train Epoch: 3 [92800/120000 (77%)]\tLoss: 0.230707\n",
      "Train Epoch: 3 [93440/120000 (78%)]\tLoss: 0.667960\n",
      "Train Epoch: 3 [94080/120000 (78%)]\tLoss: 0.480241\n",
      "Train Epoch: 3 [94720/120000 (79%)]\tLoss: 0.539872\n",
      "Train Epoch: 3 [95360/120000 (79%)]\tLoss: 0.392763\n",
      "Train Epoch: 3 [96000/120000 (80%)]\tLoss: 0.677506\n",
      "Train Epoch: 3 [96640/120000 (81%)]\tLoss: 0.677670\n",
      "Train Epoch: 3 [97280/120000 (81%)]\tLoss: 0.694506\n",
      "Train Epoch: 3 [97920/120000 (82%)]\tLoss: 0.465523\n",
      "Train Epoch: 3 [98560/120000 (82%)]\tLoss: 0.683118\n",
      "Train Epoch: 3 [99200/120000 (83%)]\tLoss: 0.752463\n",
      "Train Epoch: 3 [99840/120000 (83%)]\tLoss: 0.678621\n",
      "Train Epoch: 3 [100480/120000 (84%)]\tLoss: 0.761486\n",
      "Train Epoch: 3 [101120/120000 (84%)]\tLoss: 0.479283\n",
      "Train Epoch: 3 [101760/120000 (85%)]\tLoss: 0.615627\n",
      "Train Epoch: 3 [102400/120000 (85%)]\tLoss: 0.684523\n",
      "Train Epoch: 3 [103040/120000 (86%)]\tLoss: 0.405962\n",
      "Train Epoch: 3 [103680/120000 (86%)]\tLoss: 0.691164\n",
      "Train Epoch: 3 [104320/120000 (87%)]\tLoss: 0.754104\n",
      "Train Epoch: 3 [104960/120000 (87%)]\tLoss: 0.467686\n",
      "Train Epoch: 3 [105600/120000 (88%)]\tLoss: 0.613244\n",
      "Train Epoch: 3 [106240/120000 (89%)]\tLoss: 0.324537\n",
      "Train Epoch: 3 [106880/120000 (89%)]\tLoss: 0.606419\n",
      "Train Epoch: 3 [107520/120000 (90%)]\tLoss: 0.740389\n",
      "Train Epoch: 3 [108160/120000 (90%)]\tLoss: 0.393415\n",
      "Train Epoch: 3 [108800/120000 (91%)]\tLoss: 0.630665\n",
      "Train Epoch: 3 [109440/120000 (91%)]\tLoss: 0.616580\n",
      "Train Epoch: 3 [110080/120000 (92%)]\tLoss: 0.776698\n",
      "Train Epoch: 3 [110720/120000 (92%)]\tLoss: 0.616711\n",
      "Train Epoch: 3 [111360/120000 (93%)]\tLoss: 0.477543\n",
      "Train Epoch: 3 [112000/120000 (93%)]\tLoss: 0.536836\n",
      "Train Epoch: 3 [112640/120000 (94%)]\tLoss: 0.388267\n",
      "Train Epoch: 3 [113280/120000 (94%)]\tLoss: 0.469040\n",
      "Train Epoch: 3 [113920/120000 (95%)]\tLoss: 0.680455\n",
      "Train Epoch: 3 [114560/120000 (95%)]\tLoss: 0.254951\n",
      "Train Epoch: 3 [115200/120000 (96%)]\tLoss: 0.389086\n",
      "Train Epoch: 3 [115840/120000 (97%)]\tLoss: 0.676577\n",
      "Train Epoch: 3 [116480/120000 (97%)]\tLoss: 0.607536\n",
      "Train Epoch: 3 [117120/120000 (98%)]\tLoss: 0.830080\n",
      "Train Epoch: 3 [117760/120000 (98%)]\tLoss: 0.470166\n",
      "Train Epoch: 3 [118400/120000 (99%)]\tLoss: 0.631053\n",
      "Train Epoch: 3 [119040/120000 (99%)]\tLoss: 0.402426\n",
      "Train Epoch: 3 [119680/120000 (100%)]\tLoss: 0.615840\n",
      "\n",
      "Test set: Average loss: 0.0354, Accuracy: 100000/120000 (83%)\n",
      "\n",
      "Train Epoch: 4 [0/120000 (0%)]\tLoss: 0.539658\n",
      "Train Epoch: 4 [640/120000 (1%)]\tLoss: 0.473955\n",
      "Train Epoch: 4 [1280/120000 (1%)]\tLoss: 0.610256\n",
      "Train Epoch: 4 [1920/120000 (2%)]\tLoss: 0.404820\n",
      "Train Epoch: 4 [2560/120000 (2%)]\tLoss: 0.612265\n",
      "Train Epoch: 4 [3200/120000 (3%)]\tLoss: 0.674879\n",
      "Train Epoch: 4 [3840/120000 (3%)]\tLoss: 0.398115\n",
      "Train Epoch: 4 [4480/120000 (4%)]\tLoss: 0.905743\n",
      "Train Epoch: 4 [5120/120000 (4%)]\tLoss: 0.472408\n",
      "Train Epoch: 4 [5760/120000 (5%)]\tLoss: 0.595732\n",
      "Train Epoch: 4 [6400/120000 (5%)]\tLoss: 0.542719\n",
      "Train Epoch: 4 [7040/120000 (6%)]\tLoss: 0.614713\n",
      "Train Epoch: 4 [7680/120000 (6%)]\tLoss: 0.472306\n",
      "Train Epoch: 4 [8320/120000 (7%)]\tLoss: 0.618562\n",
      "Train Epoch: 4 [8960/120000 (7%)]\tLoss: 0.397207\n",
      "Train Epoch: 4 [9600/120000 (8%)]\tLoss: 0.686008\n",
      "Train Epoch: 4 [10240/120000 (9%)]\tLoss: 0.614549\n",
      "Train Epoch: 4 [10880/120000 (9%)]\tLoss: 0.534991\n",
      "Train Epoch: 4 [11520/120000 (10%)]\tLoss: 0.607835\n",
      "Train Epoch: 4 [12160/120000 (10%)]\tLoss: 0.321152\n",
      "Train Epoch: 4 [12800/120000 (11%)]\tLoss: 0.401322\n",
      "Train Epoch: 4 [13440/120000 (11%)]\tLoss: 0.541885\n",
      "Train Epoch: 4 [14080/120000 (12%)]\tLoss: 0.621728\n",
      "Train Epoch: 4 [14720/120000 (12%)]\tLoss: 0.388816\n",
      "Train Epoch: 4 [15360/120000 (13%)]\tLoss: 0.537534\n",
      "Train Epoch: 4 [16000/120000 (13%)]\tLoss: 0.395321\n",
      "Train Epoch: 4 [16640/120000 (14%)]\tLoss: 0.541336\n",
      "Train Epoch: 4 [17280/120000 (14%)]\tLoss: 0.396205\n",
      "Train Epoch: 4 [17920/120000 (15%)]\tLoss: 0.558455\n",
      "Train Epoch: 4 [18560/120000 (15%)]\tLoss: 0.475530\n",
      "Train Epoch: 4 [19200/120000 (16%)]\tLoss: 0.471607\n",
      "Train Epoch: 4 [19840/120000 (17%)]\tLoss: 0.392814\n",
      "Train Epoch: 4 [20480/120000 (17%)]\tLoss: 0.628017\n",
      "Train Epoch: 4 [21120/120000 (18%)]\tLoss: 0.468700\n",
      "Train Epoch: 4 [21760/120000 (18%)]\tLoss: 0.392722\n",
      "Train Epoch: 4 [22400/120000 (19%)]\tLoss: 0.611493\n",
      "Train Epoch: 4 [23040/120000 (19%)]\tLoss: 0.623144\n",
      "Train Epoch: 4 [23680/120000 (20%)]\tLoss: 0.475026\n",
      "Train Epoch: 4 [24320/120000 (20%)]\tLoss: 0.396812\n",
      "Train Epoch: 4 [24960/120000 (21%)]\tLoss: 0.612480\n",
      "Train Epoch: 4 [25600/120000 (21%)]\tLoss: 0.754822\n",
      "Train Epoch: 4 [26240/120000 (22%)]\tLoss: 0.477881\n",
      "Train Epoch: 4 [26880/120000 (22%)]\tLoss: 0.828693\n",
      "Train Epoch: 4 [27520/120000 (23%)]\tLoss: 0.542475\n",
      "Train Epoch: 4 [28160/120000 (23%)]\tLoss: 0.719916\n",
      "Train Epoch: 4 [28800/120000 (24%)]\tLoss: 0.404968\n",
      "Train Epoch: 4 [29440/120000 (25%)]\tLoss: 0.404004\n",
      "Train Epoch: 4 [30080/120000 (25%)]\tLoss: 0.755086\n",
      "Train Epoch: 4 [30720/120000 (26%)]\tLoss: 0.555047\n",
      "Train Epoch: 4 [31360/120000 (26%)]\tLoss: 0.478054\n",
      "Train Epoch: 4 [32000/120000 (27%)]\tLoss: 0.768173\n",
      "Train Epoch: 4 [32640/120000 (27%)]\tLoss: 0.540891\n",
      "Train Epoch: 4 [33280/120000 (28%)]\tLoss: 0.747810\n",
      "Train Epoch: 4 [33920/120000 (28%)]\tLoss: 0.754220\n",
      "Train Epoch: 4 [34560/120000 (29%)]\tLoss: 0.613033\n",
      "Train Epoch: 4 [35200/120000 (29%)]\tLoss: 0.465436\n",
      "Train Epoch: 4 [35840/120000 (30%)]\tLoss: 0.596487\n",
      "Train Epoch: 4 [36480/120000 (30%)]\tLoss: 0.546116\n",
      "Train Epoch: 4 [37120/120000 (31%)]\tLoss: 0.542201\n",
      "Train Epoch: 4 [37760/120000 (31%)]\tLoss: 0.688413\n",
      "Train Epoch: 4 [38400/120000 (32%)]\tLoss: 0.752538\n",
      "Train Epoch: 4 [39040/120000 (33%)]\tLoss: 0.764586\n",
      "Train Epoch: 4 [39680/120000 (33%)]\tLoss: 0.398032\n",
      "Train Epoch: 4 [40320/120000 (34%)]\tLoss: 0.690553\n",
      "Train Epoch: 4 [40960/120000 (34%)]\tLoss: 0.538293\n",
      "Train Epoch: 4 [41600/120000 (35%)]\tLoss: 0.463270\n",
      "Train Epoch: 4 [42240/120000 (35%)]\tLoss: 0.767884\n",
      "Train Epoch: 4 [42880/120000 (36%)]\tLoss: 0.408842\n",
      "Train Epoch: 4 [43520/120000 (36%)]\tLoss: 0.545174\n",
      "Train Epoch: 4 [44160/120000 (37%)]\tLoss: 0.614069\n",
      "Train Epoch: 4 [44800/120000 (37%)]\tLoss: 0.748355\n",
      "Train Epoch: 4 [45440/120000 (38%)]\tLoss: 0.488475\n",
      "Train Epoch: 4 [46080/120000 (38%)]\tLoss: 0.761378\n",
      "Train Epoch: 4 [46720/120000 (39%)]\tLoss: 0.538796\n",
      "Train Epoch: 4 [47360/120000 (39%)]\tLoss: 0.397934\n",
      "Train Epoch: 4 [48000/120000 (40%)]\tLoss: 0.616331\n",
      "Train Epoch: 4 [48640/120000 (41%)]\tLoss: 0.470443\n",
      "Train Epoch: 4 [49280/120000 (41%)]\tLoss: 0.687204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [49920/120000 (42%)]\tLoss: 0.324877\n",
      "Train Epoch: 4 [50560/120000 (42%)]\tLoss: 0.320994\n",
      "Train Epoch: 4 [51200/120000 (43%)]\tLoss: 0.469026\n",
      "Train Epoch: 4 [51840/120000 (43%)]\tLoss: 0.395729\n",
      "Train Epoch: 4 [52480/120000 (44%)]\tLoss: 0.677617\n",
      "Train Epoch: 4 [53120/120000 (44%)]\tLoss: 0.403907\n",
      "Train Epoch: 4 [53760/120000 (45%)]\tLoss: 0.542370\n",
      "Train Epoch: 4 [54400/120000 (45%)]\tLoss: 0.696572\n",
      "Train Epoch: 4 [55040/120000 (46%)]\tLoss: 0.392662\n",
      "Train Epoch: 4 [55680/120000 (46%)]\tLoss: 0.609457\n",
      "Train Epoch: 4 [56320/120000 (47%)]\tLoss: 0.396112\n",
      "Train Epoch: 4 [56960/120000 (47%)]\tLoss: 0.543836\n",
      "Train Epoch: 4 [57600/120000 (48%)]\tLoss: 0.763188\n",
      "Train Epoch: 4 [58240/120000 (49%)]\tLoss: 0.686493\n",
      "Train Epoch: 4 [58880/120000 (49%)]\tLoss: 0.774356\n",
      "Train Epoch: 4 [59520/120000 (50%)]\tLoss: 0.333403\n",
      "Train Epoch: 4 [60160/120000 (50%)]\tLoss: 0.610663\n",
      "Train Epoch: 4 [60800/120000 (51%)]\tLoss: 0.330625\n",
      "Train Epoch: 4 [61440/120000 (51%)]\tLoss: 0.831348\n",
      "Train Epoch: 4 [62080/120000 (52%)]\tLoss: 0.615747\n",
      "Train Epoch: 4 [62720/120000 (52%)]\tLoss: 0.541689\n",
      "Train Epoch: 4 [63360/120000 (53%)]\tLoss: 0.681460\n",
      "Train Epoch: 4 [64000/120000 (53%)]\tLoss: 0.544165\n",
      "Train Epoch: 4 [64640/120000 (54%)]\tLoss: 0.615170\n",
      "Train Epoch: 4 [65280/120000 (54%)]\tLoss: 0.479686\n",
      "Train Epoch: 4 [65920/120000 (55%)]\tLoss: 0.612316\n",
      "Train Epoch: 4 [66560/120000 (55%)]\tLoss: 0.401158\n",
      "Train Epoch: 4 [67200/120000 (56%)]\tLoss: 0.536409\n",
      "Train Epoch: 4 [67840/120000 (57%)]\tLoss: 0.968320\n",
      "Train Epoch: 4 [68480/120000 (57%)]\tLoss: 0.546625\n",
      "Train Epoch: 4 [69120/120000 (58%)]\tLoss: 0.472406\n",
      "Train Epoch: 4 [69760/120000 (58%)]\tLoss: 0.474131\n",
      "Train Epoch: 4 [70400/120000 (59%)]\tLoss: 0.541925\n",
      "Train Epoch: 4 [71040/120000 (59%)]\tLoss: 0.542226\n",
      "Train Epoch: 4 [71680/120000 (60%)]\tLoss: 0.618401\n",
      "Train Epoch: 4 [72320/120000 (60%)]\tLoss: 0.692456\n",
      "Train Epoch: 4 [72960/120000 (61%)]\tLoss: 0.692145\n",
      "Train Epoch: 4 [73600/120000 (61%)]\tLoss: 0.760726\n",
      "Train Epoch: 4 [74240/120000 (62%)]\tLoss: 0.543895\n",
      "Train Epoch: 4 [74880/120000 (62%)]\tLoss: 0.542127\n",
      "Train Epoch: 4 [75520/120000 (63%)]\tLoss: 0.401132\n",
      "Train Epoch: 4 [76160/120000 (63%)]\tLoss: 0.753840\n",
      "Train Epoch: 4 [76800/120000 (64%)]\tLoss: 0.818151\n",
      "Train Epoch: 4 [77440/120000 (65%)]\tLoss: 0.400139\n",
      "Train Epoch: 4 [78080/120000 (65%)]\tLoss: 0.468843\n",
      "Train Epoch: 4 [78720/120000 (66%)]\tLoss: 0.761240\n",
      "Train Epoch: 4 [79360/120000 (66%)]\tLoss: 0.614503\n",
      "Train Epoch: 4 [80000/120000 (67%)]\tLoss: 0.325875\n",
      "Train Epoch: 4 [80640/120000 (67%)]\tLoss: 0.625996\n",
      "Train Epoch: 4 [81280/120000 (68%)]\tLoss: 0.536116\n",
      "Train Epoch: 4 [81920/120000 (68%)]\tLoss: 0.676433\n",
      "Train Epoch: 4 [82560/120000 (69%)]\tLoss: 0.546675\n",
      "Train Epoch: 4 [83200/120000 (69%)]\tLoss: 0.613244\n",
      "Train Epoch: 4 [83840/120000 (70%)]\tLoss: 0.543758\n",
      "Train Epoch: 4 [84480/120000 (70%)]\tLoss: 0.820754\n",
      "Train Epoch: 4 [85120/120000 (71%)]\tLoss: 0.757896\n",
      "Train Epoch: 4 [85760/120000 (71%)]\tLoss: 0.468995\n",
      "Train Epoch: 4 [86400/120000 (72%)]\tLoss: 0.916550\n",
      "Train Epoch: 4 [87040/120000 (73%)]\tLoss: 0.541659\n",
      "Train Epoch: 4 [87680/120000 (73%)]\tLoss: 0.684420\n",
      "Train Epoch: 4 [88320/120000 (74%)]\tLoss: 0.546383\n",
      "Train Epoch: 4 [88960/120000 (74%)]\tLoss: 0.477062\n",
      "Train Epoch: 4 [89600/120000 (75%)]\tLoss: 0.321305\n",
      "Train Epoch: 4 [90240/120000 (75%)]\tLoss: 0.837127\n",
      "Train Epoch: 4 [90880/120000 (76%)]\tLoss: 0.398357\n",
      "Train Epoch: 4 [91520/120000 (76%)]\tLoss: 0.397428\n",
      "Train Epoch: 4 [92160/120000 (77%)]\tLoss: 0.688849\n",
      "Train Epoch: 4 [92800/120000 (77%)]\tLoss: 0.397820\n",
      "Train Epoch: 4 [93440/120000 (78%)]\tLoss: 1.046569\n",
      "Train Epoch: 4 [94080/120000 (78%)]\tLoss: 0.683990\n",
      "Train Epoch: 4 [94720/120000 (79%)]\tLoss: 0.615251\n",
      "Train Epoch: 4 [95360/120000 (79%)]\tLoss: 0.618144\n",
      "Train Epoch: 4 [96000/120000 (80%)]\tLoss: 0.390989\n",
      "Train Epoch: 4 [96640/120000 (81%)]\tLoss: 0.391684\n",
      "Train Epoch: 4 [97280/120000 (81%)]\tLoss: 0.608397\n",
      "Train Epoch: 4 [97920/120000 (82%)]\tLoss: 0.604304\n",
      "Train Epoch: 4 [98560/120000 (82%)]\tLoss: 0.473123\n",
      "Train Epoch: 4 [99200/120000 (83%)]\tLoss: 0.384555\n",
      "Train Epoch: 4 [99840/120000 (83%)]\tLoss: 0.541642\n",
      "Train Epoch: 4 [100480/120000 (84%)]\tLoss: 0.463623\n",
      "Train Epoch: 4 [101120/120000 (84%)]\tLoss: 0.692643\n",
      "Train Epoch: 4 [101760/120000 (85%)]\tLoss: 0.396658\n",
      "Train Epoch: 4 [102400/120000 (85%)]\tLoss: 0.615547\n",
      "Train Epoch: 4 [103040/120000 (86%)]\tLoss: 0.692760\n",
      "Train Epoch: 4 [103680/120000 (86%)]\tLoss: 0.243892\n",
      "Train Epoch: 4 [104320/120000 (87%)]\tLoss: 0.618611\n",
      "Train Epoch: 4 [104960/120000 (87%)]\tLoss: 0.390805\n",
      "Train Epoch: 4 [105600/120000 (88%)]\tLoss: 0.843387\n",
      "Train Epoch: 4 [106240/120000 (89%)]\tLoss: 0.698695\n",
      "Train Epoch: 4 [106880/120000 (89%)]\tLoss: 0.695880\n",
      "Train Epoch: 4 [107520/120000 (90%)]\tLoss: 0.613306\n",
      "Train Epoch: 4 [108160/120000 (90%)]\tLoss: 0.475284\n",
      "Train Epoch: 4 [108800/120000 (91%)]\tLoss: 0.757912\n",
      "Train Epoch: 4 [109440/120000 (91%)]\tLoss: 0.894471\n",
      "Train Epoch: 4 [110080/120000 (92%)]\tLoss: 0.272624\n",
      "Train Epoch: 4 [110720/120000 (92%)]\tLoss: 0.339402\n",
      "Train Epoch: 4 [111360/120000 (93%)]\tLoss: 0.613671\n",
      "Train Epoch: 4 [112000/120000 (93%)]\tLoss: 0.688496\n",
      "Train Epoch: 4 [112640/120000 (94%)]\tLoss: 1.197852\n",
      "Train Epoch: 4 [113280/120000 (94%)]\tLoss: 0.265257\n",
      "Train Epoch: 4 [113920/120000 (95%)]\tLoss: 0.613012\n",
      "Train Epoch: 4 [114560/120000 (95%)]\tLoss: 0.403319\n",
      "Train Epoch: 4 [115200/120000 (96%)]\tLoss: 0.613749\n",
      "Train Epoch: 4 [115840/120000 (97%)]\tLoss: 0.472042\n",
      "Train Epoch: 4 [116480/120000 (97%)]\tLoss: 0.691963\n",
      "Train Epoch: 4 [117120/120000 (98%)]\tLoss: 0.542674\n",
      "Train Epoch: 4 [117760/120000 (98%)]\tLoss: 0.616674\n",
      "Train Epoch: 4 [118400/120000 (99%)]\tLoss: 0.607405\n",
      "Train Epoch: 4 [119040/120000 (99%)]\tLoss: 0.614469\n",
      "Train Epoch: 4 [119680/120000 (100%)]\tLoss: 0.254370\n",
      "\n",
      "Test set: Average loss: 0.0386, Accuracy: 100000/120000 (83%)\n",
      "\n",
      "Total time taken: 2025 seconds\n"
     ]
    }
   ],
   "source": [
    "# use the resnet50 model provided by pytorch with pre-trained parameter set to true\n",
    "# detach the final FC layer of Resnet-50 and attach a layer with 200 output nodes (number of classes in tiny-imagenet)\n",
    "### YOUR CODE STARTS HERE ####\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 200)\n",
    ")\n",
    "#print(model)\n",
    "\n",
    "### YOUR CODE ENDS HERE ####\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "## Define Adam Optimiser with a learning rate of 0.01 (You should add the FC layer parameters only)\n",
    "### YOUR CODE STARTS HERE ####\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "### YOUR CODE ENDS HERE ####\n",
    "\n",
    "start = timeit.default_timer()\n",
    "for epoch in range(1, 5):\n",
    "    train(model, device, train_dataloader, criterion, optimizer, epoch)\n",
    "    test(model, device, test_dataloader, criterion)\n",
    "stop = timeit.default_timer()\n",
    "print('Total time taken: {} seconds'.format(int(stop - start)) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-e8W4HtbeG4Q"
   },
   "source": [
    "#### Question-3\n",
    "\n",
    "Report the final test accuracy displayed above (If you are not getting the exact number shown in options, please report the closest number).\n",
    "\n",
    "1. 83%\n",
    "2. 35%\n",
    "3. 70%\n",
    "4. 94%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2PsUKoKHc3Xv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DL4CV-Assignment-3-Week-5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "carnd-gpu1",
   "language": "python",
   "name": "carnd-gpu1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
